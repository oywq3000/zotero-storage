Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation
Alexander Raistrick∗, Lingjie Mei∗, Karhan Kayan∗, (∗equal contribution; random order) David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, Jia Deng Department of Computer Science, Princeton University
Abstract
We introduce Infinigen Indoors, a Blender-based procedural generator of photorealistic indoor scenes. It builds upon the existing Infinigen system, which focuses on natural scenes, but expands its coverage to indoor scenes by introducing a diverse library of procedural indoor assets, including furniture, architecture elements, appliances, and other day-to-day objects. It also introduces a constraint-based arrangement system, which consists of a domain-specific language for expressing diverse constraints on scene composition, and a solver that generates scene compositions that maximally satisfy the constraints. We provide an export tool that allows the generated 3D objects and scenes to be directly used for training embodied agents in realtime simulators such as Omniverse and Unreal. Infinigen Indoors is open-sourced under the BSD license. Please visit infinigen.org for code and videos.
1. Introduction
Synthetic data rendered by conventional computer graphics has seen increasing adoption in computer vision[13, 44, 47, 48, 52, 55, 68] and AI research[25, 33, 83], especially for 3D vision[31, 51, 78, 80–82, 91] and embodied AI[16, 37, 42, 75, 77, 95]. Synthetic data can be rendered in unlimited quantities and can automatically provide highquality 3D ground truth, enabling large-scale training of computer vision models and embodied agents. Notably, many state-of-the-art 3D vision systems[79, 82] and robotic systems [39, 45] have been trained purely in simulation yet perform surprisingly well in the real world zero-shot. A promising direction for creating synthetic data is procedural generation, which uses mathematical rules to create 3D objects and scenes, as opposed to manual sculpting or real-world scanning. These mathematical rules can have parameters that are randomized to allow infinite variations. For example, trees can be generated through a recursive set of rules that randomly branch off. Compared to reusing a fixed, static set of 3D assets, procedural generation can greatly
improve the diversity of the synthetic data and the simulated environments. Infinigen [67] is a recent work that pushed the idea of procedural generation to the limit. Infinigen is an opensource system that generates photorealistic 3D scenes fully procedurally, meaning that every 3D asset, from shape to material, from large structures to small details, is completely procedural, without using any external static asset. Being fully procedural means that every aspect of the 3D scene, from the details of individual objects to their arrangements in a scene, can be customized and controlled by simply modifying the underlying mathematical rules. As a result, a 3D scene can be randomized at all levels down to the smallest details, as opposed to only at the level of object arrangement, which was common in earlier work that used procedural generation. However, the current Infinigen system is limited to natural scenes and objects (terrains, animals, plants, etc.). Although natural scenes could be sufficient for training foundation models as evidenced by natural evolution [67], this hypothesis remains unproven and may require additional advances in learning algorithms and architecture designs. Evidence from the current literature suggests that synthetic training data that more closely approximates the application domain is still likely to lead to better downstream performance. To overcome this limitation, we introduce Infinigen Indoors, a procedural generator of photorealistic indoor scenes. It expands the coverage of Infinigen to indoor scenes, which are relevant for many high-impact applications including robotics and augmented reality. Infinigen Indoors generates diverse indoor objects, including furniture, appliances, cookware, dining utensils, architectural elements, and other common day-to-day objects. It also generates full indoor scenes, including the interior of multi-room, multi-floor buildings, with object arrangements that are physically and semantically plausible. Fig. 1 shows random samples of generated scenes, and Fig. 2 shows some automatic annotations. Like the original Infinigen, Infinigen Indoors is not a fixed set of 3D models or scenes; instead, it is an opensource generator that can create unlimited variations both
arXiv:2406.11824v1 [cs.CV] 17 Jun 2024


Figure 1. Random, non cherry-picked sample of images generated by our system. From top left to bottom right, we show images from dining rooms, bathrooms, living rooms and kitchens. Please see Appendix B for an extended random sample.
at the object level and at the scene level. Infinigen Indoors is also 100% procedural, using no external assets and using only mathematical rules to generate everything from scratch. Infinigen Indoors builds upon the original Infinigen and Blender [9] but makes significant new contributions. The main contributions include (1) a library of procedural generators of indoor assets, (2) a constraint-based arrangement system, (3) a tool to export the generated scenes to real-time simulators such as NVIDIA Omniverse [60] and Unreal Engine[17]. Our second contribution—a constraint-based arrangement system—offers a new capability specifically targeting indoor settings. Indoor scenes are artificial, and object arrangement exhibits a greater degree of regularity than natural scenes: for example, furniture usually does not block the entrance of a room. We thus develop a system that lets the user specify scene arrangement constraints through a domain-specific language using a set of Python APIs. The constraints cover many types of common arrangement, including symmetry (“Place chairs symmetrically around the table”), spatial relation (“Place plant pots close to windows ”), quantity (“An equal number of knives and forks”), physics (“Ensure vases do not overhang”), and accessibility (“Ensure there is freespace in-front of all appliances”). The constraints can be understood as a type of declarative procedural rules that express what the user desires but not how to achieve it. Like other procedural rules, the constraints can be randomized and can be customized by the user. In addition to constraint specification, our arrangement
system also includes a constraint solver, which searches for an arrangement that maximally satisfies a set of given constraints. Our solver greedily performs simulated annealing on whole-house floor plans, followed by large furniture layouts and then small objects. Compared to existing approaches for scene arrangement, our solver is highly expressive, supporting complex compositional constraints that are challenging or infeasible for existing approaches. In addition, it is the first solver integrated with an open-source and fully procedural generator. Our constraint-based arrangement system is a significant contribution because it vastly improves the generation system’s usability and customizability. Because it separates constraint specification from constraint solving, a user can conveniently express the objectives of procedural generation without worrying about implementation. This capability is not available in the original Infinigen, where the user has to customize the procedure rules at the implementation level. Our third contribution—exporting to real-time simulators—is also noteworthy because it allows the generated 3D objects and scenes to be directly used for training embodied agents in real-time simulators such as Omniverse. Thus, Infinigen Indoors can supply diverse 3D assets for simulation environments and enhance their domain randomization. To validate the effectiveness of the generated data and demonstrate our system’s unique customizability, we use Infinigen Indoors to generate synthetic data for shadow removal and occlusion boundary detection, two tasks that lack


Dataset Arrangement Procedural Provides # Scenes # Assets Free External Asset Source
Method Assets Procedural Code in Total in Total Assets
3DSSG [86] Real-world scans No N/A 1.5K 48K Yes 3RScan [85] Matterport3D [7] Real-world scans No N/A 2K [64] - Yes None Stanford 2D-3D-S [3] Real-world scans No N/A 270 - Yes None ScanNet [11] Real-world scans No N/A 1.5K - Yes None SceneNN [32] Real-world scans No N/A 100 - Yes None OpenRooms [50] Real-world scans No No 1.3K 3K [67] No ($500) ShapeNet [8], Scan2CAD [4], Adobe Stock [35] Replica [76] Real-world scans No N/A 18 - Yes None Structured3D [4] Artist layouts No N/A 22K 472K No Professional Designers Hypersim [70] Artist layouts No N/A 461 59K No ($6000) Evermotion Architectures [18] InteriorNet [49] Artist layouts No N/A 22M 1M No Manufactures / Kujiale [43] Habitat 3.0 [40] Artist layouts No N/A 211 18.7K Yes Floorplanner [19], Proffesional Designers 3D-FRONT [22] Artist layouts No N/A 19K 13K Yes 3D-FUTURE [23] Robotrix [24] Artist layouts No N/A 16 - No UE4Arch [84], UnrealEngine Marketplace [36] DeepFurniture [53] Artist layouts No N/A 20K - No Adobe Mixamo [34] SceneNetRGBD [104] Obj. Cat. Dist. No N/A ∞ 5.1K Yes SceneNet [28], ShapeNet [8] LUMINOUS [103] Hierarchical Sampling No Yes ∞ 2K Yes AI2-THOR [42] SceneNet [29] Optimizer No No ∞ 3.7K Yes 3DModelFree [20], ModelNet [94], Archive3D [1], Stanford database ProcTHOR [13] Procedural Rules No Yes ∞ 1.6K Yes AI2-THOR [42], Professional Designers Holodeck [98] LLM No Yes ∞ 50K Yes Objaverse [12] Aria [58] Procedural Rules No No 100K 8K - 
Infinigen Indoors (Ours) Constraint Language Yes Yes ∞ ∞ Yes None
Table 1. Comparisons to existing datasets and generators. Many existing datasets/generators use external, static asset libraries and have limited number of scenes. Ours is fully procedural, without using any external source. Dashes represent numbers we could not acquire or estimate.
a
TTeexxtt
(a) (b) (c)
(d) (e) (f)
(g) (h) (i)
Figure 2. Each image (a) is rendered from a mesh (b), from which we can also extract Depth (c), Surface Normals (d), Occlusion Boundaries (e), Segmentation (f), Bounding Boxes (e) and Optical Flow (h), with Albedo (i) from rendering metadata.
abundant existing training data. Our experiments show that data from our system improves generalization performance on indoor scenes. Like the original Infinigen, Infinigen Indoors will be opensourced under the BSD license to enable free and unlimited use by everyone, and to enable community contributions of additional procedural generators.
2. Related Work
We provide a detailed comparison of Infinigen Indoors with existing datasets and generators in Tab. 1. Real-world datasets. Various real-world datasets have been introduced for indoor scene understanding [3, 7, 11, 32, 73, 74, 86], including the earlier and widely used NYUv2 [73] and Sun RGB-D [74], as well as more recent datasets [3, 5, 11, 86]. However, real-world datasets are labor-intensive to collect and limited in size. In addition, real-world 3D
ground truth can be difficult to acquire due to the limitations of depth sensors, which include limited resolution and range, errors with transparent/reflective surfaces, and artifacts at object edges. Synthetic Indoor Datasets. There are many existing synthetic datasets for indoor scenes [4, 22, 24, 29, 40, 42, 49, 53, 70, 76]. However, the underlying 3D assets of many datasets are not freely accessible, limiting their utility. In addition, most use a static library of 3D assets, limiting their diversity. Recent work [13, 103] has incorporated procedural generation for scene layout and floor plan generation, but still relies on static libraries of objects and materials. In contrast, Infinigen Indoors is 100% procedural, with all assets from shape to texture generated from scratch with unlimited variation.
Object arrangement and layout generation.
Constraints are potent tools to describe the layout of a scene. Early works like [96] represent constraints as hardcoded programs, and [56] represent them as physical relations. Data-driven works like [46, 62, 63, 69, 90, 92] learn constraints implicitly from data. Such implicit constraints are less customizable, interpretable, and controllable than Infinigen Indoors. Recently, modeling constraints using probabilistic graphs have become more popular: [97] uses pairwise grouping, while [14, 100] further extends it to spatial and hierarchical constraints. [99] uses factor graphs to parse the constraints, while [57] models them with Bayesian network. [65, 102] formulates constraints as potentials Markov Random Fields on a fixed graph, which capture only noncompositional and associative constraints for rooms and objects. Compared to existing systems, ours is the first to integrate directly with procedural object generators, and our constraint language is higher level and more easily extend


Figure 3. Random samples of procedurally generated doors (top), staircases (middle/bottom) and windows/warehouse shelving (bottom-right).
Figure 4. Random samples of procedurally generated ovens, dishwasher and sinks (top/middle), living-room furniture (middle) and bathroom fixtures (bottom).
Figure 5. Random samples of procedurally generated furniture, including sofa, chairs, and beds (top), tables (middle/bottom), and shelves (bottom).
able than existing systems. Our system specifies high level goals for abstract classes of objects (e.g. ’furniture’, ’storage’), rather than exhaustive distance/angle distributions for specific objects [100] which must be fitted to example scenes. Our language also supports compositional constraints such as “place glassware only on shelves against a dining-room wall.” These features allow users to write new constraints for their specific needs, including domains without existing artist-made scenes. Our constraint solver uses simulated annealing, following prior work [100], but involves moves that are unique to our constraint language, including updates to object-object relations or changing the parameters of procedural objects (e.g. the size of a table).
3. Method
3.1. Procedural Asset Generation
All assets used in Infinigen Indoors are generated from scratch by compact probabilistic programs. These programs
Figure 6. Random samples of procedurally generated tableware, including dinnerware (Row 1-2), cookware (Row 2), food containers (Row 3) and dining utensils (Row 4).
Figure 7. Random samples of procedurally generated home decorations, including lamps, hardware, balloons, wall decor (top), rugs, book stacks, vases, and plants (bottom). Small assets for decoration purposes, usually attached to the ground or walls.
Figure 8. A collection of materials generated in Infinigen Indoors. The first figure shows one material per generator, with columns (1-3) used on assets of various sizes, (4-5) used on assets and rooms, and (6) for abstract art and text. The second figure shows multiple materials from the same generator with different parameters.
have many human-controllable parameters, which are randomized by default, or can be manually overridden by the user. These parameters are used along with additional lowlevel random noise to generate meshes via geometry nodes, modifiers, or mesh manipulations in Blender. We provide a total of 79 randomized procedural object generators. By category, we cover Appliances (10 generators, 112 params), Windows/Doors/Staircases (14 generators, 127 params), Furniture (17 generators, 216 params), Decorations (15 generators, 92 params), and Small Objects (19 generators, 194 params). See Appendix F for a list.


Figure 9. Example usage of our constraint specification API, specifying the quantity and aesthetic constraints for a dining table and chairs.
Architectural elements shown in Fig. 3 are integrated into room as fixtures. We use array repetition of atomic components to build staircases, and mesh booleaning to cut out the panels of doors and windows. Large objects shown in Fig. 4 and 5 provide assets related to cooking, seating and storage. We use soft-body collision simulation to model soft blankets, clothing, and stuffed pillows when they are put on a supporting surface. Small objects shown in Fig. 6 and 7 can be attached to support surfaces, walls, or ceilings. We also devised a combined text-and-shape logo generator that produces procedural texture for fabrics, food packaging, and art decor. We use cloth simulation to inflate balloons and food packaging with air. Materials are all procedurally created with Blender’s shader nodes, as shown in Fig. 8. We provide 30 material generators with 120 controllable parameters in total, split approximately evenly between types of wood, ceramic, fabric, metal, and others. We cover 78% of OpenSurfaces’[6] material categories, up from 21% for Infinigen.
3.2. Constraint Specification API
Indoor scene layouts are highly regular, and follow complex rules governing ergonomics, aesthetics, and functionality. Moreover, the rules that apply to a particular object depend on context - for example, tables are placed against walls when used as desks in study rooms, but must be far from walls and surrounded by chairs when used in a dining room. To capture this, we provide a high-level Constraint Specification API, which allows the user to write expressive objective functions to describe the properties of a desirable scene. An example of the Constraint Specification API is shown in 9. Each constraint in our API is a compute graph of geometric, set filtering, and arithmetic operations. Geometric operators are designed to compute spatial and geometric properties, including minimum distance, rotational and reflection symmetry, angle alignment, 2D free-space, accessibility and volume or area of objects. Each geometric operator accepts a set of objects as input, which can be provided by
filtering the scene using semantics and scene graph relations. This allows the user to create scoped constraints that apply only to objects attached to specific surfaces or rooms. Additionally, these geometry terms are affected by the parameters (length, width, etc.) of the procedurally generated assets, meaning that optimizers can automatically discover optimal furniture parameters given available space and constraints. Our system features common scalar arithmetic operations, comparisons, and forall / sum operators to gather results over sets of objects. Please see Appendix A.1 for the full API and more examples of constraint specifications. A more concrete example of our constraint language can be found in Fig. 9, where the constraint program specifies common-sense human ergonomics and semantic relations found in residential homes. This constraint graph has a total of 1058 nodes, which compute 11 hard constraints and 25 score terms (soft constraints). We provide example constraint specifications for living rooms, bathrooms, dining rooms, kitchens, and warehouses. We believe that many users will consider creating custom constraints tailored to particular applications when generating training data. Our constraint system is designed to allow easy customization. Our initial spec. has avg. 15 constraints specific to each room: approx. 15 lines of Python. We believe this cost is very tractable when users need customization.
3.3. Arrangement Solver
Because our Constraint Specification API is flexible, the solver needs to search a prohibitively large space in which finding an exact minimum is impossible. To deal with this, it uses Simulated Annealing[41] with Metropolis-Hastings criterion [30, 59]. The solver first takes the current state s and randomly chooses a move category. It then uses the constraint graph to generate a proposed state s′ that can be reached using the move. The current and proposed states are evaluated on the graph specified by the provided constraints and score terms, yielding loss terms l(s) and l(s′). Then, the solver calculates the transition probability between s and s′ as
p(s′|s) = min
"
exp l(s) − l(s′)
τ
!
,1
#
where τ is the temperature of the solver, which cools exponentially from τ = 0.25 to τ = 0.001. Our solver allows both discrete and continuous moves: Addition - Adds a procedural object to the scene. Deletion - Deletes an object from the scene. Relation Plane Change - Assigns an object to another plane. Resample - Regenerates an object with new parameters. Reinitialize Pose - Samples a new random pose for an object. Translate - Translates the object within its DoF plane. Rotate - Rotates the object around its DoF axis. We observe that not all moves are equally significant at each point in the optimization. In an empty scene, object


Figure 10. In Fig. a), we show ten randomly generated single-story floor plans with a diverse set of room combinations, connectedness, and overall contours. In Fig. b), we show results for the generation of multistory floor plans. Floors 0,1,2 are displayed separately. Staircases connect adjacent floors. We remove exterior walls and ceilings for visibility.
Figure 11. Qualitative room arrangement results, grouped by room type. From left to right, we show bathrooms, dining rooms, living rooms and kitchens.
addition and relation change allow for higher loss reduction, whereas in a cluttered scene, continuous object movement allows for higher loss reduction. Thus, we provide a schedule for moves so that probabilities for discrete moves decay gradually and probabilities for continuous moves increase.
The objects in an indoor scene are interdependent on each other, which makes it unfeasible to optimize over all of them simultaneously. However, they usually depend on each other hierarchically (e.g. a cup is on the table, which is on the floor). To exploit this hierarchy, we divide our optimization into three stages: large object optimization, medium object optimization, and small object optimization.
Each object is constrained in its movement due to the constraints specified by the user and the discrete moves proposed by the solver. For instance, a bookshelf that is stable against a wall is only allowed to move along the 1D line between the wall and the floor. Consequently, an object’s degrees of freedom (DoFs) for rotation and translation are determined based on its relations to other objects. When the solver samples a continuous move, it restricts the object’s motion to these DoFs. When the solver samples a discrete move, it places the object in the constrained subspace.
Floorplan-specific solver and constraints Our floorplan generator creates realistic full-house room meshes, as shown in Fig. 10. First, we procedurally generate a room adjacency graph specifying the number, type, and connectedness of individual rooms required in the floor plan. This graph is produced by inference on a probabilistic contextfree grammar on room types, or can be wholly or partially derived from user input. We define our objective function as a weighted combination of the terms below, and optimize it using simulated annealing subject to constraints from the room adjacency graph. See Appendix D for full definitions.
• Shortest path to entrance • Typical room area • Room aspect ratio • Room convexity • Room wall conciseness • Functional room area • Room collinearity • Narrow passages • Exterior length by room • Exterior corners by room • Staircase occupancies • Staircase IOU with rooms
We initialize our floorplan solver by generating a random house outline, and subdividing it using a Mondrian Process [72] until it produces sufficient spaces for each room. We extrude a wall segment inwards or outwards at each step, or swap the assignment subject to the room adjacency graph. Either action will lead to a change in loss, which we convert


Figure 12. Two scenes imported into Unreal Engine 5 (above) and Omniverse Isaac Sim (below). Unreal Engine runs at 60 FPS, and Isaac Sim, with physics simulation enabled, runs at 50 FPS, both on RTX 4090s.
Figure 13. Qualitative zero-shot results on SRD [66] test dataset.
to an acceptance probability using Metropolis-Hasting as in Sec 3.3. Once solving is complete, we add floor, wall, and ceiling materials, doors, windows, and staircases, all subject to constraints based on room type and adjacency. Fig. 11 shows the solutions to room arrangements with objects placed inside.
3.4. Data Export
All Image Shadow Region Non-Sh Region
Test Set Model PSNR↑ RMSE↓ PSNR↑ RMSE↓ PSNR↑ RMSE↓
ISTD [89]
R 31.96 4.27 38.04 6.58 34.13 3.85 R+S 31.72 4.30 37.41 7.06 33.80 3.85
SRD [66]
R 22.83 10.84 25.59 18.75 27.93 7.68 R+S 24.56 9.54 27.39 16.63 29.37 6.67
Table 2. Shadow removal task quantitative performance on ISTD, ISTD+, and SRD dataset across the three variations of the model.
We develop a one-click tool to export assets from Infinigen Indoors to real-time simulators, using Universal Scene
Image GT Infinigen-Nature [67] Ours
Figure 14. Qualitative results on synthetic artistic scenes [26].
Description (USD) or other formats. As seen in Fig. 12, indoor scenes can be exported to Omniverse Isaac Sim and Unreal Engine 5 and can be run at interactive frame rates. This exporting capability allows Infinigen Indoors to help train embodied agents in virtual environments. Infinigen Indoors uses Blender’s procedural material system, which is by default not portable to other simulators or scene editors. To resolve this, we provide tools to automatically post-process and UV-map entire indoor scenes, and use texture baking to create standard texture maps for material color, roughness, metallicity and more. We also provide export code to convert single objects to textured OBJ, FBX or STL meshes, and automatically generate collision and articulation information as Universal Robot Description Format (URDF) files.
4. Experiments
4.1. Solver Performance
To efficiently solve large numbers of constraints in cluttered scenes, we optimized our solver with various features for faster convergence. Plane hashing enables faster access to bounding planes during discrete optimization. BVH caching enables faster mesh distance and collision calculations by reusing Bounding-Volume-Hierarchies except when mutated by a state update. Evaluation caching maintains a cache of results in the evaluation graph. Move filtering narrows down the search space in continuous optimization by selectively pruning candidates to those that can reduce the loss. Placeholder optimization only generates full meshes when other objects are assigned to them; otherwise it keeps bounding boxes. To analyze the importance of these features, we conducted an ablation in Tab. 3 and Fig. 15, which shows solver performance with each feature removed. All results are averaged over 20 random scenes with 5k solver steps. Our full system provides a ≈ 3x speedup compared to the non-optimized version. Most of the performance gains come from BVH caching and Plane Hashing. We observe that discrete changes such as pose re-initialization, relation changes, and object resampling are necessary for compelling visuals, but decrease the quantitative score and increase the runtime. When we run our fully optimized system as long as the non-optimized version we get a 28% score increase. Perceptual Study We performed a crowdsourced human


(a) (b) (c) (d)
Figure 15. Qualitative Ablation. From left to right, we show scenes generated by our system with 10K solving steps (a), 1K solving steps (b), with collision checking removed (c) and with symmetry terms disabled (d)
Method Runtime ↓ Avg. Score (s) ↑ #Objects ↑
Full System 2280.68 80.84 28.94 w/o Discrete change 1872.60 86.20 30.82 w/o Eval. cache 2242.79 80.84 28.94 w/o Move Filter 2633.95 84.75 45.06 w/o Placeholder optim. 2665.18 80.46 27.71 w/o Plane Hashing 3522.86 92.65 34.12 w/o BVH cache 4740.48 80.84 29.06 Full System 100 min. 6080.84 114.72 49.65 w/o Optimization 6308.92 89.43 47.76
Table 3. Ablation of solver performance optimizations.
evaluation of our scenes and layouts, following metrics from ATISS [63]. Table 9 shows that the subjects preferred Infinigen Indoors over [14, 63, 69, 92] in terms of both realism, layout realism, and the lack of errors, although we recognize “realism” may also have been influenced by asset and lighting quality. See section H for more details.
4.2. Shadow Removal
To demonstrate its flexibility in data generation, we used Infinigen-Indoors to create a dataset consisting of 2k image pairs of shadow and shadow-free variants. These pairs were generated by toggling the shadow property of lighting within Blender. For each pair, shadow masks were produced using Otsu’s thresholding [61] method. We use ShadowFormer [27] model for the experiments and consider two variations: only trained on ISTD [89] real dataset (R), and trained on combination of ISTD and 2k Infinigen Indoors synthetic dataset (R+S). The results are shown in Tab. 2. While using synthetic data leads to slightly worse performance on the ISTD dataset, the zero-shot application on the SRD [66] dataset shows clear improvement for generalization to new test datasets. Qualitative results are shown in Fig. 13.
4.3. Occlusion Boundary Estimation
To validate the effectiveness of Infinigen Indoors, we also evaluate on the task of occlusion boundary estimation, a task with limited available data. We produce 1464 images annotated with ground truth. We train three U-Net [71] models from scratch separately on these images, on images generated from Infinigen [67] and Hypersim [70]. We then compare their performance on a curated test set of photorealistic artist-designed synthetic 3D scenes for architecture
Training Dataset ODS OIS mAP
Infinigen-Nature [67] 14.38 19.43 10.80 Hypersim [70] 26.02 19.44 15.69 Infinigen Indoors (Ours) 29.47 30.29 19.09
Table 4. Occlusion boundary quantitative results on a curated test set of photorealistic artist-designed synthetic 3D scenes for architecture visualization [26].
visualization [26], since no existing photorealistic indoor dataset provides such annotations. See Appendix G for more details. We report the following three metrics [87, 88, 93]: (i) optimal dataset scale F-score (ODS), representing the best F-score achieved on the dataset using a uniform threshold across all test images; (ii) optimal image scale F-score (OIS) indicating the cumulative F-score on the dataset obtained with thresholds dependent on individual images; and (iii) mean average precision (mAP) denoting the mean precision across the complete recall range. As we can see in Tab. 4, our Infinigen Indoors-trained model generalizes better. The model achieves higher performance across all metrics. These findings underscore the usefulness of Infinigen Indoors as a valuable training resource. Qualitative results are depicted in Fig. 14.
5. Contributions & Acknowledgements
Alexander Raistrick, Lingjie Mei and Karhan Kayan contributed equally and are ordered randomly. Each has the right to list their name first in their CV. Alexander Raistrick performed team coordination and developed the constraint language and object graph solver. Lingjie Mei developed the room solver and many procedural assets. Karhan Kayan developed the geometric constraints and object pose solver. David Yan developed the scene exporter and other utilities. Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Zeyu Ma and Lahav Lipson developed procedural assets and utilities. Jia Deng conceptualized and led the project, and set directions. This work was partially supported by the National Science Foundation and Amazon.


References
[1] Archive 3D. Archive 3d. https://archive3d.net/. 3 [2] Mart ́ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man ́e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi ́egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org. 18 [3] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D3D-Semantic Data for Indoor Scene Understanding. ArXiv e-prints, 2017. 3
[4] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X Chang, and Matthias Nießner. Scan2cad: Learning cad model alignment in rgb-d scans. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 2614–2623, 2019. 3 [5] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. Arkitscenes - a diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. In NeurIPS, 2021. 3 [6] Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala. OpenSurfaces: A richly annotated catalog of surface appearance. ACM Trans. on Graphics (SIGGRAPH), 32(4), 2013. 5
[7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. 3
[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. ShapeNet: An information-rich 3D model repository. arXiv preprint arXiv:1512.03012, 2015. 3
[9] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 2 [10] David F. Crouse. On implementing 2D rectangular assignment algorithms. IEEE Transactions on Aerospace and Electronic Systems, 52(4):1679–1696, 2016. Conference Name: IEEE Transactions on Aerospace and Electronic Systems. 17 [11] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richlyannotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 3 [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects, 2022. 3 [13] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. Advances in Neural Information Processing Systems, 35:59825994, 2022. 1, 3 [14] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. ArXiv, abs/2206.06994, 2022. 3, 8, 28 [15] Steven Diamond and Stephen Boyd. CVXPY: A Pythonembedded modeling language for convex optimization. Journal of Machine Learning Research, 17(83):1–5, 2016. 18 [16] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. pages 1–16, 2017. 1 [17] Epic Games. Unreal engine. 2 [18] Evermotion. Evermotion architectures. https : / / evermotion.org/shop. 3
[19] Floorplanner. Floorplanner.com. https://floorplanner. com/. 3
[20] 3D Model Free. 3d model free. http : / / www . 3dmodelfree.com/. 3
[21] Frank M. Frey, Aaron Robertson, and Michael Bukoski. A method for quantifying rotational symmetry. New Phytologist, 175(4):785–791, 2007. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.14698137.2007.02146.x. 13 [22] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1093310942, 2021. 3 [23] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3D-FUTURE: 3D furniture shape with texture. IJCV, 129(12):3313–3337, 2021. 3 [24] Alberto Garcia-Garcia, Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez, and Alvaro Jover-Alvarez. The robotrix: An extremely photorealistic and very-large-scale indoor dataset of sequences with robot trajectories and interactions. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6790–6797. IEEE, 2018. 3 [25] John T Guibas, Tejpal S Virdi, and Peter S Li. Synthetic medical images from dual generative adversarial networks. arXiv preprint arXiv:1709.01872, 2017. 1
[26] Gumroad. Gumroad. https://discover.gumroad. com/. 7, 8, 26, 28 [27] Lanqing Guo, Siyu Huang, Dingshuo Liu, Hao Cheng, and Bihan Wen. Shadowformer: Global context helps image shadow removal. ArXiv, abs/2302.01650, 2023. 8, 27


[28] Ankur Handa, Viorica Pa ̆tra ̆ucean, Simon Stent, and Roberto Cipolla. Scenenet: An annotated model generator for indoor scene understanding. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 5737–5743. IEEE, 2016. 3 [29] Ankur Handa, Viorica Pa ̆tra ̆ucean, Simon Stent, and Roberto Cipolla. Scenenet: An annotated model generator for indoor scene understanding. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 5737–5743, 2016. 3 [30] W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97109, 1970. eprint: https://academic.oup.com/biomet/articlepdf/57/1/97/23940249/57-1-97.pdf. 5 [31] Rasmus Laurvig Haugaard and Anders Glent Buch. Surfemb: Dense and continuous correspondence distributions for object pose estimation with learnt surface embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6749–6758, 2022. 1 [32] Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and Sai-Kit Yeung. Scenenn: A scene meshes dataset with annotations. In 2016 fourth international conference on 3D vision (3DV), pages 92–101. Ieee, 2016. 3 [33] Braden Hurl, Krzysztof Czarnecki, and Steven Waslander. Precise synthetic image and lidar (presil) dataset for autonomous vehicle perception. In 2019 IEEE Intelligent Vehicles Symposium (IV), pages 2522–2529. IEEE, 2019. 1 [34] Adobe Inc. Adobe mixamo. https://www.mixamo.com, . 3
[35] Adobe Inc. Adobe stock. https://stock.adobe.com/ 3d-assets, . 3
[36] Epic Games Inc. Unreal engine marketplace. https:// www.unrealengine.com/marketplace/en- US/store, .3 [37] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. RLBench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019–3026, 2020. 1 [38] R. Jonker and A. Volgenant. A shortest augmenting path algorithm for dense and sparse linear assignment problems. Computing, 38(4):325–340, 1987. 17 [39] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Mueller, Vladlen Koltun, and Davide Scaramuzza. Champion-level drone racing using deep reinforcement learning. Nature, 620:982–987, 2023. 1 [40] Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Schacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel X Chang, and Manolis Savva. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. arXiv preprint arXiv:2306.11290, 2023. 3
[41] Scott Kirkpatrick. Optimization by simulated annealing: Quantitative studies. Journal of Statistical Physics, 34(5-6): 975–986, 1984. 5 [42] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An interactive 3D
environment for visual AI. arXiv preprint arXiv:1712.05474, 2017. 1, 3 [43] Kujiale. Kujiale.com. https://www.kujiale.com/. 3 [44] Hei Law and Jia Deng. Label-free synthetic pretraining of object detectors. arXiv preprint arXiv:2208.04268, 2022. 1 [45] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science Robotics, 5, 2020. 1
[46] Kurt Leimer, Paul Guerrero, Tomer Weiss, and Przemyslaw Musialski. LayoutEnhancer: Generating Good Indoor Layouts from Imperfect Data. In SIGGRAPH Asia 2022 Conference Papers, pages 1–8, 2022. arXiv:2202.00185 [cs]. 3
[47] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded recurrent network with adaptive correlation. In CVPR, 2022. 1 [48] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded recurrent network with adaptive correlation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16263–16272, 2022. 1 [49] Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark, Dimos Tzoumanikas, Qing Ye, Yuzhong Huang, Rui Tang, and Stefan Leutenegger. Interiornet: Mega-scale multisensor photo-realistic indoor scenes dataset. arXiv preprint arXiv:1809.00716, 2018. 3
[50] Zhengqin Li, Ting Yu, Shen Sang, Sarah Wang, Sai Bi, Zexiang Xu, Hong-Xing Yu, Kalyan Sunkavalli, Milovs Havsan, Ravi Ramamoorthi, and Manmohan Chandraker. Openrooms: An open framework for photorealistic indoor scene datasets. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7186–7195, 2020. 3 [51] Lahav Lipson, Zachary Teed, and Jia Deng. RAFT-Stereo: Multilevel recurrent field transforms for stereo matching. In International Conference on 3D Vision (3DV), 2021. 1
[52] Lahav Lipson, Zachary Teed, Ankit Goyal, and Jia Deng. Coupled iterative refinement for 6d multi-object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6728–6737, 2022. 1 [53] Bingyuan Liu, Jiantao Zhang, Xiaoting Zhang, Wei Zhang, Chuanhui Yu, and Yuan Zhou. Furnishing your room by what you see: An end-to-end furniture set retrieval framework with rich annotated benchmark dataset. arXiv preprint arXiv:1911.09299, 2019. 3
[54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017. 27
[55] Zeyu Ma, Zachary Teed, and Jia Deng. Multiview stereo with cascaded epipolar raft. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXI, pages 734–750. Springer, 2022. 1


[56] John McCormac, Ankur Handa, Stefan Leutenegger, and Andrew J. Davison. Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation? 2017 IEEE International Conference on Computer Vision (ICCV), pages 2697–2706, 2017. 3 [57] Paul C. Merrell, Eric Schkufza, and Vladlen Koltun. Computer-generated residential building layouts. ACM SIGGRAPH Asia 2010 papers, 2010. 3
[58] META. Project aria. https://www.projectaria.com/ datasets/ase/. 3
[59] Nicholas C. Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, and A. H. Teller. Equation of state calculations by fast computing machines. Journal of Chemical Physics, 21:1087–1092, 1953. 5
[60] NVIDIA. Omniverse. https://developer.nvidia. com/omniverse. 2
[61] Nobuyuki Otsu. A threshold selection method from graylevel histograms. IEEE Transactions on Systems, Man, and Cybernetics, 9(1):62–66, 1979. 8 [62] Wamiq Para, Paul Guerrero, Tom Kelly, Leonidas Guibas, and Peter Wonka. Generative Layout Modeling using Constraint Graphs, 2020. arXiv:2011.13417 [cs]. 3 [63] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregressive transformers for indoor scene synthesis. In Advances in Neural Information Processing Systems, pages 1201312026. Curran Associates, Inc., 2021. 3, 8, 28 [64] Akshay Gadi Patil, Supriya Gadi Patil, Manyi Li, Matthew Fisher, Manolis Savva, and Hao Zhang. Advances in DataDriven Analysis and Synthesis of 3D Indoor Scenes, 2023. arXiv:2304.03188 [cs]. 3 [65] Siyuan Qi, Yixin Zhu, Huang Siyuan, Chenfanfu Jiang, and Song Zhu. Human-centric indoor scene synthesis using stochastic grammar. 2018. 3 [66] Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson W. H. Lau. Deshadownet: A multi-context embedding deep network for shadow removal. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2308–2316, 2017. 7, 8, 27 [67] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Ankit Goyal, Kaiyu Yang, and Jia Deng. Infinite photorealistic worlds using procedural generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12630–12641, 2023. 1, 3, 7, 8, 26, 27, 28 [68] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In ECCV, pages 102–118. Springer, 2016. 1 [69] Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models, 2018. arXiv:1811.12463 [cs]. 3, 8, 28 [70] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. In
Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10912–10922, 2021. 3, 8, 28 [71] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015. 8, 28 [72] Daniel M. Roy and Yee Whye Teh. The mondrian process. In Neural Information Processing Systems, 2008. 6, 20
[73] Nathan Silberman and Rob Fergus. Indoor scene segmentation using a structured light sensor. In 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), pages 601–608, 2011. 3 [74] Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 3
[75] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martı ́n-Martı ́n, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. pages 477–490, 2022. 1 [76] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 3
[77] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 1 [78] Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402–419. Springer, 2020. 1 [79] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. In Neural Information Processing Systems, 2021. 1
[80] Zachary Teed and Jia Deng. DROID-SLAM: Deep visual SLAM for monocular, stereo, and RGB-D cameras. In NeurIPS, 2021. 1
[81] Zachary Teed and Jia Deng. Raft-3d: Scene flow using rigid-motion embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8375–8384, 2021.


[82] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. arXiv preprint arXiv:2208.04726, 2022. 1
[83] Vajira Thambawita, Pegah Salehi, Sajad Amouei Sheshkal, Steven A Hicks, Hugo L Hammer, Sravanthi Parasa, Thomas de Lange, Pål Halvorsen, and Michael A Riegler. Singan-seg: Synthetic training data generation for medical image segmentation. PloS one, 17(5):e0267976, 2022. 1 [84] UE4Arch. Ue4arch. https://ue4arch.com/. 3
[85] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Niessner. Rio: 3d object instance re-localization in changing indoor environments. In Proceedings IEEE International Conference on Computer Vision (ICCV), 2019. 3
[86] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3
[87] Chaohui Wang, Huan Fu, Dacheng Tao, and Michael J Black. Occlusion boundary: A formal definition & its detection via deep exploration of context. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(5):2641–2656, 2020. 8
[88] Guoxia Wang, Xiaochuan Wang, Frederick WB Li, and Xiaohui Liang. Doobnet: Deep object occlusion boundary detection from an image. In Computer Vision–ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2–6, 2018, Revised Selected Papers, Part VI 14, pages 686–702. Springer, 2019. 8
[89] Jifeng Wang, Xiang Li, and Jian Yang. Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. In CVPR, 2018. 7, 8
[90] Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X. Chang, and Daniel Ritchie. Planit: planning and instantiating indoor scenes with relation graph and spatial prior networks. ACM Trans. Graph., 38:132:1–132:15, 2019. 3
[91] Wenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tartanvo: A generalizable learning-based vo. In Conference on Robot Learning, pages 1761–1772. PMLR, 2021. 1
[92] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nießner. Sceneformer: Indoor scene generation with transformers. 2021 International Conference on 3D Vision (3DV), pages 106–115, 2020. 3, 8, 28
[93] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Learning to detect motion boundaries. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2578–2586, 2015. 8
[94] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912–1920, 2015. 3
[95] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. SAPIEN: A simulated part-based interactive environment. In CVPR, 2020. 1
[96] Ken Xu, James Stewart, and Eugene Fiume. Constraintbased automatic placement for scene composition. In Graphics Interface, 2002. 3
[97] Wenzhuo Xu, Bin Wang, and Dong-Ming Yan. Wall grid structure for interior scene synthesis. Comput. Graph., 46: 231–243, 2015. 3 [98] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, and Christopher Clark. Holodeck: Language guided generation of 3d embodied ai environments, 2024. 3 [99] Yi-Ting Yeh, Lingfeng Yang, Matthew Watson, Noah D. Goodman, and Pat Hanrahan. Synthesizing open worlds with constraints using locally annealed reversible jump mcmc. ACM Transactions on Graphics (TOG), 31:1 – 11, 2012. 3 [100] Lap-Fai Craig Yu, Sai-Kit Yeung, Chi-Keung Tang, Demetri Terzopoulos, Tony F. Chan, and S. Osher. Make it home: automatic optimization of furniture arrangement. ACM SIGGRAPH 2011 papers, 2011. 3, 4
[101] Hagit Zabrodsky, Shmuel Peleg, and David Avnir. Continuous symmetry measures. Journal of the American Chemical Society, 114(20):7843–7851, 1992. Publisher: American Chemical Society. 13 [102] Yizhou Zhao, Kaixiang Lin, Zhiwei Jia, Qiaozi Gao, Govind Thattai, Jesse Thomason, and Gaurav S. Sukhatme. Luminous: Indoor scene generation for embodied ai challenges. ArXiv, abs/2111.05527, 2021. 3 [103] Yizhou Zhao, Kaixiang Lin, Zhiwei Jia, Qiaozi Gao, Govind Thattai, Jesse Thomason, and Gaurav S Sukhatme. Luminous: Indoor scene generation for embodied ai challenges. arXiv preprint arXiv:2111.05527, 2021. 3
[104] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3d: A large photo-realistic dataset for structured 3d modeling. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16, pages 519–535. Springer, 2020. 3


Appendix
A. Constraint Specification API
A.1. API Description
with semantics (also notated as operator squarebrackets for brevity) extracts the subset of a set of objects that satisfies a semantic predicate, e.g. ”extract the subset of rooms which are dining-rooms” or ”extract the subset of furniture objects which are shelving”. The hierarchy of these predicates is defined by asset creators, or can be reconfigured by constraint program writers if they intend to use an object for an unusual purpose. Using hierarchical classes for this filtering operation allows every constraint to apply to the most broad class of objects possible, to avoid rewriting or restating constraints for objects that fulfill a similar function.
related to extracts the subset of a set of objects related to any member of a second set of objects via some relation. The exact way in which the objects are related is userconfigurable by passing in any parameterized Relation object from the options below (StableAgainst, SupportedBy), which represents a predicate that can be True or False of any pair of objects. By combining related to with other filtering operations, the user can express constraints on arbitrarily complex contexts such as ”maximize the number of dining chairs against dining tables inside of rooms adjacent to kitchens”, to encourage plenty of seating near food preparation areas, etc.
scene retrieves the set of all objects currently in the scene. All constraint programs are ultimately functions of the current scene state, so this node serves as the leaf node of all constraint program expressions (besides numeric constants). Users rarely place constraints on scene directly. Instead, we expect the user first to take subsets via the operations above.
StableAgainst specifies a relation using a child object’s planar surface, a parent object’s planar surface, and a margin between the surfaces. It checks that the child’s surface is parallel to the parent’s, the child is not overhanging, and the child’s surface is exactly at the specified margin. A concrete example would be specifying the sofa as stable against the floor with zero margin and stable against the wall with a 10cm margin. Alternatively, specifying a painting to be stable against the wall ensures that the painting does not overhang across the edge of the wall.
SupportedBy specifies a relation using a child object’s planar surface and a parent object’s planar surface. It means that the child object would not fall over from the parent object. More precisely, the surfaces are parallel against
each other with zero margins, and the centroid of the child object is contained within the convex hull of the intersection between the child and the parent object. The last condition is to ensure zero torque by gravity. An example use case is a coffee cup teetering on the table’s edge. In this case, the cup is supported by the table, but it is not stable against it since it is overhanging.
count returns the cardinality of a set of objects in the scene.
area, volume returns the total area or volume of the bounding boxes of objects in a set. We use bounding boxes to avoid expensive calculations to compute the exact volume of each mesh, and we find this serves as a suitable proxy to incentivize larger assets. Area always takes over the two largest axes of an object and is usually used for 2D objects like paintings or rugs.
min distance calculates the minimum distance between two sets of tagged objects. For instance, the minimum distance between the walls and the back of the couch. The minimum distance is defined as the distance between the closest two points on the two sub-meshes identified by the tags.
angle alignment cost quantifies how far a group of objects are from being angle aligned to a reference object on the XY plane. The cost is calculated as
X
i
1 − cos θi 2
where θi is the angular difference between the front-facing normal of object i and the inward normal of the closest edge of the reference object. The contribution of each object is in the [0, 1] range. An example use case is minimizing the angle alignment cost between chairs and tables to make the chairs face the table. Another example is using an alignment score to align furniture to the walls in order to give the arrangement a more grid-like shape.
rotation asymmetry gives a continuous characterization of the rotational asymmetry of a set of objects based on [21, 101]. It measures the deviation of the set of objects from a regular polygon with perfectly rotationally symmetric orientations. From another perspective, it measures the rotational asymmetry of a set of point-vector pairs. The score consists of two parts and is calculated as
score = location asymmetry + orientation asymmetry
2.


Figure 16. Random, non-cherry-picked sample of procedurally generated residential homes (Part 1 of 2)


Figure 17. Random, non-cherry-picked sample of procedurally generated residential homes (Part 2 of 2)


Figure 18. Constraint Program for whole residential homes. Left shows hard constraints, right shows continous objective scores. Our system flexibly composes API calls to apply any constraint to any class of object describable within the scene.
Suppose the location of the ith object is given by ⃗xi and there are n objects. The location asymmetry is calculated as follows:
• Let ⃗pi = ⃗xi − c where c is the centroid of the objects. • Rotate all ⃗pi so that ⃗p1 is aligned with the axis. • Normalize ⃗pi by dividing by maxi ||⃗pi||. • Let ⃗fi be vector ⃗pi rotated by −2iπ/n. • Compute ⃗q as the average of ⃗fi. • Let ⃗wi be vector ⃗q rotated by 2iπ/n. • Then, we have location asymmetry = 1
n
P ||⃗wi − ⃗pi||2.
The orientation asymmetry score follows the same steps as the location asymmetry, but with ⃗pi replaced by the frontal plane normal of the object i.
As an example, rotational asymmetry score can be used to encourage tableware being rotationally symmetric on the table not only with respect to their location but also their orientation. It can also be used to make chairs rotationally symmetric around the table.
reflection asymmetry calculates a continuous reflectional asymmetry score for a set of objects relative to a reference object. This score quantifies the deviation of objects from mirror symmetry. The process involves reflecting each object across a plane and then comparing the original and reflected objects. From another perspective, it quantifies the mirror asymmetry of a set of point-vector pairs. The asymmetry score is computed as follows:
• Determine Reflection Plane: Identify the plane of reflection, which can be any of the median planes of the bounding box of the reference object. • Reflect Objects: The objects Oi are represented by (⃗pi, qi) where ⃗pi is the object’s location and qi is the object’s orientation. Each object Oi is reflected across the plane to obtain its mirror image O′
i. The reflection of a point ⃗p
is given by ⃗p′ = ⃗p − 2(⃗p · ⃗Nplane) ⃗Nplane. The reflection of an axis-angle represented orientation θ⃗e is given by θ′ ⃗e′
where ⃗e′ = ⃗e − 2(⃗e · ⃗Nplane) ⃗Nplane and θ′ = −θ.


Optimization objectives based on... Our API Function Example Usage (Described in Natural Language)
Objects with (abstract) semantics with semantics a.k.a. [ ] Scope a constraint to a hierarchical class e.g. shelves, storage, all furniture, or all objects Objects related to other objects related to Cooking pots go in the center when on tables, but can go anywhere on a countertop. Objects on arbitrary surfaces SupportedBy Multi-story homes, decorations on shelves, countertops, fridges Whether objects overhang StableAgainst Paintings cant overhang walls Non-convex object shapes Yes, procedural placeholders Objects go inside shelves, chairs tuck under table
Variable quantity of objects count Allow between 0 and 3 sofas in a living-room, but as many as possible Size of objects area, volume Generate the biggest possible TV & Sofa that fits well Pair-wise distances min distance Place dining tables & ceiling lights far from walls Pair-wise angle difference angle alignment Align tables to parallel to the nearest wall Symmetry around an object rotational asymmetry Chairs should be rotationally symmetric when placed around circular tables Symmetry across a plane reflection asymmetry Bed-side tables should be symmetric on either side of a bed Objects facing other objects focus score Sofas should face TVs or paintings Object accessibility accessibility cost Leave space in front of sofas / appliances Empty space on a surface freespace 2d Leave some space leftover in room / on countertop
Arbitrary arithmetic / nonlinearities + - * / pow hinge Encourage certain ratio of ceiling-lights to room-area Boolean comparisons / logic == < <= and in range Ensure there are 2 to 6 chairs for every table
every object must satisfy a predicate all Every bookcase must have ≥ 10 books sum/mean across specific objects mean sum Compute average distance to wall over many objects, rather than minimum
Table 5. Capabilities included in our API. Please see Sec. B and C for example programs, and surrounding text for full descriptions. For our API, functions can be composed arbitrarily, e.g. scene.with semantics(...).related to(...).count().hinge(...) to create a nonlinear objective w.r.t. number of objects in a certain context.
• Bipartite Matching: A cost-minimizing bipartite matching is performed between the set of original objects {Oi} and their reflected counterparts {O′
i } to find the optimal pairings based on a cost matrix derived from positional and angular deviations. We use a modified JonkerVolgenant algorithm for this step [10, 38]. • Calculate Deviations: – Positional Deviation: For each paired object (Oi, O′
i) =
((⃗pi, qi), ( ⃗p′i, q′
i)), calculate the Euclidean distance
Dpos = ||⃗pi − ⃗p′i||.
– Angular Deviation: Calculate the angular difference Dang = 2 arccos(|qi · q′
i |), where qi and q′
i are the quaternion representations of the paired objects’ orientations. • Weight Deviations: Each deviation is weighted by a factor V(Oi), which is the volume of the bounding box of Oi. The weighted deviation for each object pair is Ddev(Oi) = V(Oi) × (Dpos + Dang).
• Normalization: The total deviation is normalized by a factor α, which is the average distance between objects: α= 1
N(N−1)
Pi, j ||⃗pi − ⃗p′i||, where N is the number of objects.
• Compute Asymmetry Score: The reflectional asymmetry score is derived as
Score = 1 − 1
1 + Pi Ddev(Oi)/α
This reflection score is useful in contexts such as encouraging chairs to be symmetric around a long rectangular table, or encouraging furniture to have mirror symmetry for visual appeal, or encouraging paintings to be symmetrical across the room.
accessibility cost computes how much a set of objects B block access to a set of objects A. We offer two versions. In the fast version, the function selects the closest object in B to each object in A based on the centroid distance. In the slow version, it finds the closest point on any mesh in B to each mesh in A. The mathematical formulation can be described as follows: We first take the projection of a’s centroid onto its specified plane (frontal plane by default) by
⃗aproj = ⃗ac − (⃗ac − ⃗fp) · ⃗na ⃗na
where ⃗ac is the centroid of object a, ⃗fp is a point on the specified plane, and ⃗na is the normal vector of the specified plane. For a given object a ∈ A, we define ⃗b(a) and ⃗bclosest pt. The fast version defines
⃗b(a) = arg min
b∈B
∥⃗bc − ⃗aproj∥
⃗bclosest pt = Centroid of the selected ⃗b(a)
The slow version defines
⃗b(a) = Object in B with the point closest to mesh a
⃗bclosest pt = Point on mesh ⃗b(a) closest to mesh a
For both fast and slow versions, the accessibility cost is calculated as
cost = X
a∈A
(⃗bclosest pt − ⃗aproj) · ⃗na
∥⃗bclosest pt − ⃗aproj∥2
× ∥⃗b(a)d∥


where ⃗b(a)d is the diagonal vector of the bounding box of the chosen object ⃗b(a). We note that the accessibility cost increases as the blocking object gets larger, as the blocking objects get closer, and as the blocking object is more in front of the specified plane. An example usage of accessibility cost is when we want to penalize objects being directly in front of TVs, paintings, or closets.
focus score encourages focusing a set of objects A on an object b. It is calculated as
X
a∈A
1 − ⃗na · (⃗bc − ⃗ac)
2||⃗bc − ⃗ac||
where ⃗na is the front facing normal of object a. The vectors ⃗ac, ⃗bc denote the centroids of a and b respectively. The contribution of each object is in the [0, 1] range. An example use case of focus score is focusing the sofas on the TV to encourage a more realistic layout. Another example use case is focusing a set of seats on a round table.
freespace 2d returns the amount of 2D free space available on a set of objects A after accounting for the space occupied by objects B. It is calculated as
X
a∈A
μ(proj(a)) − X
b∈B
μ(proj(b))
where proj is the projection to the XY plane and μ gives the area of a 2D shape. An example use case is minimizing the free space on a table to encourage placing more objects on the table, or maximizing the free space in a living room to make it less cluttered.
Arithmetic / non-linearities provide basic scalar arithmetic and an implementation of the standard hinge loss function, all computed using the standard Python definitions. The exact set of mathematical operators provided here is not critical; our system treats scalar losses as a black box, so any arbitrary Python math expressions are acceptable.
Boolean comparisons allow equality or inequality checking between values, usually for creating hard constraints on cardinalities or distances. When used to check the size of a set, our system will use the constraint statement to inform what Addition moves are proposed as explained in “Cardinality Bounding”.
all provides control flow logic akin to the “forall” ∀ symbol as used in formal proofs. It is commonly used in constructing soft/hard constraints. We avoid allowing arbitrary Python control flow (for loops, if statements, etc.) as it
makes symbolic reasoning difficult by restricting the user to symbolic expressions. This design decision is similar to other compute graph programming frameworks (e.g. Tensorflow [2], CVXPY [15]). For example, by forcing the user to use a symbolic ’all’ statement rather than a for loop, we can make inferences such as ”if all chairs go near tables, and there must be at least one chair, then there must be at least one table”, which allows the user to write higher level and fewer constraints, with the system deducing all logical consequences.
Forall statements take as input a loop variable name, and a constraint program that contains the loop variable as a leaf node at one or more locations. During execution, the child constraint program is substituted with the real values of the loop variable and evaluated to obtain the various results.
mean, sum compute the standard scalar mean and sum operations, using similar control flow logic and evaluation substitution mechanisms as all as described above.
B. Extended Random Sample & Constraint Code for Residential Scenes
Please inspect Fig. 16 and Fig. 17 for an extended random sample of our main residential home generator (as shown in Fig. 1).
These images were derived from the constraint code designed for residential homes, shown in Figure 18. This code uses a total of 105 soft and hard constraints, with 19 for dining rooms, 14 for living rooms, 9 for bathrooms, 18 for kitchens, 16 for warehouses, and 30 which apply abstractly to all rooms. These constraints are used to cover object assignments (object A goes on object B), ratios (numbers of chairs per table, objects per shelf), stability (TV placed against the wall; objects don’t overhang unsupported), distance (plants placed near window), and more.
C. Extension to warehouse scenes
To show the generality of our solving system, we implemented a simple constraint program that uses existing language features to specify the high-level objectives of a warehouse environment, with furniture on shelves and smaller items on wooden pallets. See Fig. 19 for the full program. Various further extensions are possible, for example indicating a preference for larger objects to be placed lower or higher on the shelves, or certain objects to be placed near the front of the warehouse / store. We show example images in Fig. 20, as well as a topdown view showing only the shelving and lighting layout.


Figure 19. Constraint program for warehouse scenes. In only a few high-level statements, we specify the hierarchy of allowed objects, and competing placement objectives that give rise to an appropriate shelf and object layout for any warehouse scene.
D. Floorplan Solver Details
D.1. Floor plan graph generation
We generate floor plans that have 1 to 3 floors. For each floor, we generate a floor plan graph where individual nodes rep


(a) Shelving Arrangement (b) Example Images
Figure 20. Warehouse scene arrangement (left) and example first-person images (right). Using only a few high-level objectives, we extend our existing placement system and existing furniture generators to create a hardware-store-like environment.
resent a room with a certain type, and each edge represents the connectedness of two rooms it is linked to. We support rooms of the following type: kitchen, bedroom, living-room, closet, hallway, bathroom, garage, balcony, dining-room, utility, staircase. hallway can mean any corridor or passage between rooms, and staircase means the room or space where one can find the stairs. The graph is generated by a Probabilistic Context-free Grammar(PCFG), where the graph first starts off as a single node living-room, and gradually appends zero, one or more rooms of certain types to the leaf nodes. The probability distribution that we use is shown in Tab. 6 and Tab. 7.
Room parent Room children Probability
LivingRoom LivingRoom 0.1 Bedroom Cat(0, 0.3, 0.3, 0.3, 1) Closet 0.1 Bathroom 0.4 Garage 0.4 Balcony 0.2 DiningRoom 0.8 Utility 0.2 Hallway Cat(0.5, 0.4, 0.1) Kitchen Garage 0.1 Utility 0.2 Bedroom Bathroom 0.3 Closet 0.5 Bathroom Closet 0.2 DiningRoom Kitchen 1.0 Hallway 0.2
Table 6. Probability of the number of rooms PCFG produces for each leaf node in the graph for the ground floor. Such probability is conditioned on the parent room type (Column 1) and the children room type (Column 2). The probability (Column 3) can either be a Bernoulli distribution (shown as the sole parameter) or a Categorical (Cat) distribution (shown as the probability of the number of children, starting with zero).
Additional edges are added to rooms to create a floor plan graph based on the generated tree. Additional hallways are added and shared among the children with the same parent.
Room parent Room children Probability
LivingRoom Bedroom Cat(0, 0.4, 0.5, 0.2) Closet 0.2 Bathroom 0.4 Balcony 0.4 Utility 0.2 Hallway Cat(0, 0.5, 0.5) Bedroom Bathroom 0.3 Closet 0.5 Bathroom Closet 0.2 Balcony Utility 0.4 Hallway 0.1
Table 7. Probability of the number of rooms PCFG produces for each leaf node in the graph. The annotations are similar to Tab. 6
Based on the number of floors and the current level, a porch (balcony) or staircase may also be added to the graph. All room plans that do not observe bathroom privacy (i.e. a bedroom is connected to a bathroom without going through other bedrooms) or are not planar are rejected. Based on the user input, floor plans with an incorrect number of designated rooms are also rejected. By default, we require all floor plan graphs to have at least one living room and one bathroom.
D.2. Floor plan initialization
Based on the floor plan graph for a specific floor, we first deduct an estimated contour area based on the sum of typical areas of all the rooms on one floor, which can also used to derive the width and length of the contour. To derive the contour on one floor, we randomly bevel the corners with a rectangular, round, or 45-degree profile that provides the diversity of the contour shape. Contours for floors upstairs are either the exact same copy of the contour on its lower floor or a subset of the contour on its lower floor. The spaces are subdivided from the contour following the Mondrian Process [72]. For each iteration, we randomly select a mostly rectangular space and divide it along one of its axes, and we repeat such division so that there are 1.5 times more blocks than are required in the floor plan


graph. All divisions apply by rounding off the division onto a grid with a size of 0.5, and divisions leading to a bad aspect ratio are rejected. We merge the spaces until there’s the same number of spaces as in the floor plan graph, then compute the adjacency relations of all divided spaces, where spaces are adjacent if they share an edge of size greater or equal to a threshold (to place doors). We randomly add a staircase placeholder inside the contour for multistory floor plans, which roughly indicates the location of the staircase. The staircase placeholder ensures staircases across adjacent floors are in the same spatial location. Among these contour divisions, we try to find one where the assignment of rooms suffices the floor plan graph via adjacency relations. In addition to the adjacency relations in the floor plan graph, we also ensure that all exterior-facing rooms, including the bedroom, garage, and balcony, have access to the house’s exterior. Only the divided spaces intersecting with the staircase placeholder can be assigned to the staircase room. We can find a proper assignment of rooms that satisfies the floor plan graph and other constraints via depth-first search.
D.3. Objective function for floor plan optimization
The objective function is defined on a floor plan where spaces are assigned to a node in the floor plan graph. The objective is composed of twelve constraints detailed as follows:
Shortest path to entrance constraint encourages unidirectional room access from the entrance. We compute the shortest path from all nodes to the floor’s entrance, either the front entrance for the ground floor or the staircase for rooms upstairs. The path is computed in an axis-aligned fashion and can only traverse connected rooms on the floor plan graph. The amount of detour for each path is the percentage of the path in the wrong direction of the Euclidean distance from the entrance to that room. The objective function is computed as squared detours summed across rooms. Denote F as the set of all floors, e f is the entrance on floor f , and → is the path allowed by the adjacency between rooms:
Lsp = X
f ∈F
X
r∈ f
∥e f → r∥1,direct
∥e f → r∥1
−1
!2
Typical room area constraint encourages room of typical area so that the spaces serve the best function. A list of the typical area occupied by rooms is listed in Tab. 8, which is based on a typical US household. The ideal proportion of a room is computed as the typical area of that room divided by the sum of all the room’s typical areas on that floor. The objective function is computed by the difference between a room’s ideal proportion on one floor and the room’s true proportion on that floor, summed across rooms. For all
rooms r ∈ f , we compute its ideal area as
arear = typical arear
Pr′∈ f typical arear
area f
A formula for the objective function is
Lta = X
f ∈F
X
r∈ f
max arear
arear
, arear
arear
!
Room type Typical area
Kitchen 20 Bedroom 20 LivingRoom 25 DiningRoom 20 Closet 3 Bathroom 7 Utility 3 Garage 35 Balcony 6 Hallway 6 Staircase 20
Table 8. Typical area occupied by rooms
Room aspect ratio constraint encourages rooms of certain types to be square. The objective function is computed as the difference between the true aspect ratio and one, squared and summed across rooms. Denote by Rs the rooms needed to be square, we have
Lar = X
f ∈F
X
r∈ f ∩Rs
max heightr
widthr
, widthr
heightr
!
−1
!2
Room convexity constraint encourages rooms to be overall convex. The convexity of each room is computed as the ratio between the area of the convex hull of a room and the area of the room itself. The objective function is computed as the squared difference between the convexity of a room and one, summed across rooms.
Lconv = X f ∈F
X
r∈ f
areaconvex hull(r)
arear
−1
!2
Room wall conciseness constraint encourages rooms to have fewer boundary edges, which allows rooms to have better-formed geometry along many iterations of perturbations. The objective function is the squared difference between the number of boundary edges of a room with four (minimal number of edges), summed across rooms.


Lconc = X f ∈F
X
r∈ f
(∥w ∈ wallsr∥ − 4)2
Functional Room area constraint incentivizes the available area useful for dwellings of the people inside the house, characterized by functional rooms. Functional rooms include kitchens, bedrooms, living rooms, bathrooms, and dining rooms. The objective function is computed as the proportion of the area covered by these rooms, measured in squared distance with one. Denote by R f the set of functional rooms, we have
L f unc = X f ∈F
Pr∈ f ∩Rf arear
area f
−1
!2
Room collinearity constraint incentivizes walls of multiple rooms to be collinear for aesthetics and construction purposes. The objective function measures the number of distinct X or Y coordinates for all walls of rooms across one floor.
Lcol = X f ∈F
(|{x|∃r ∈ f, x the x-coords of a wall in r}|
{y|∃r ∈ f, y the y-coords of a wall in r}
Narrow passages constraint limits the number of passages in a room (including hallways) where people or furniture may find it hard to move across. We identify a narrow passage in a room by eroding and then buffering the 2D room contour with a certain threshold margin. Narrow passages inside a room are no longer present in the room contour after that erosion-buffer operation. The objective function is measured as the difference between the area of the room contour pre- and post-erosion-buffer operation. An illustration of the erosion-buffer operation can be found in Fig. 21. The formula can be written as
Lnar = X f ∈F
X
r∈ f
arear − areaerosion-buffer(r)
Exterior length by room constraint encourages rooms of certain types to cover most of the exterior walls and windows since people would expect more views of outside in these rooms and more privacy concerns in other rooms. The exterior room types include bedrooms and balconies, denoted by Re. The objective function is evaluated using the exterior length covered by these rooms divided by the exterior length covered by all rooms on that floor, measured by its squared distance with one.
Erode
Erode
Buffer
Buffer
a)
b)
Figure 21. Illustration of narrow passage and erosion-buffer operation. a) In rooms with no narrow passage, the room contour is restored after the operation; b) In rooms with narrow passage, the narrow passage disappears from the room contour after the operation.
Lextr = X f ∈F
Pr∈ f ∩Re
Pw∈wallsr exterior ∥w∥
Pr∈ f Pw∈wallsr exterior ∥w∥ − 1
!2
Exterior corner by room constraint encourages the aforementioned room types to cover most exterior corners, which supposedly have better views. The objective function is measured by the percentage of corners covered by these rooms, measured by its squared distance with one.
Lextc = X f ∈F
Pr∈ f ∩Re |{c ∈ cornersr|c exterior}|
Pr∈ f |{c ∈ cornersr|c exterior}| − 1
!2
Staircase occupancy constraint encourages the room assigned as the staircase room to cover the staircase placeholder space. It is measured as the percentage of staircase placeholder space covered by the staircase room, measured by its squared distance with one. Denote by sp the staircase placeholder and Rs the staircase rooms, we have
Lstair occ = X f ∈F


X
r∈ f ∩Rs
areasp ∩ r areasp
−1


2
Staircase IOU constraint further encourages the room assigned as the staircase room to be exactly the same size, shape, and location as the staircase placeholder space. It is measured as the IOU of the staircase placeholder with the staircase, measured by its squared distance with one.


Lstair occ = X f ∈F


X
r∈ f ∩Rs
IOUr,sp − 1


2
D.4. Floor plan optimization moves
While solving for the aforementioned constraints, we need to design a set of moves to perturb the floor plan, which are listed as follows and illustrated in Fig. 22:
Extruding a wall segment inwards randomly select one wall segment of a room in the current floor plan and move it towards the inside of the room by one grid size (0.5). Other rooms sharing part of the wall with the selected wall will fill up the space left by the move.
Extruding a wall segment outwards randomly select one wall segment of a room in the current floor plan and move it towards the outside of the room by one grid size (0.5). Other rooms sharing part of the wall with the selected will give up their space to the room.
Swapping the assignment for adjacent rooms randomly select one space for a room and its neighbor and swap their room assignment. In all of the above moves, we reject moves that lead to a floor plan that does not suffice the floor plan graph on that floor. We also reject moves that lead to invalid geometry, including degenerate, disconnected, or out-of-boundary rooms, and those that fail to satisfy the constraints on exterior rooms and staircase placeholders. One may think of satisfying floor plan graphs as a hard constraint.
Moving staircase. We also provide an additional move for the staircase placeholder. The staircase placeholder can move along one of the axes by one grid size. At each iteration of the simulated annealing, we first select one of the floors to operate on or choose to move the staircase placeholder. Then, we randomly choose one of the three moves to apply. A move is rejected if it no longer satisfies the hard constraint given by the floor plan graph or rejected by the simulated annealing probability computed using the change in the objective function.
D.5. Postprocessing of floor plan
After acquiring the floor plan for all floors, we must convert it to a mesh. Each space assigned to a room is extruded by the height of the wall and solidified by the thickness of the wall; both parameters are the same across all rooms. Besides placing furniture, we conduct the following postprocessing operations.
a) b)
c) d)
Figure 22. All floor plan optimization moves. a) The original floor plan, with each color showing the assignment of each room (e.g. blue for living-room 0, orange for bedroom 0, and green for bedroom 1; b) floor plan after extruding rightmost wall segment of the blue room inwards; c) floor plan after extruding rightmost wall segment of the blue room outwards; d) floor plan after swapping the assignment for the green and the blue room.
Placement of doors and windows. For pairs of rooms that share an edge in the floor plan, they must share a wall segment with its length over a certain threshold. We then cut the shape of the door from both room meshes and place the door in that space. Doors can be opened towards the inside of the house or away from the house’s entrance for ergonomics. For other pairs of rooms designated by the user, i.e., between dining rooms and living rooms, one may choose to remove all the walls in between and place no doors. For rooms facing the exterior of the house, if they can have windows installed, we selectively cut off the shape of the window from the room meshes with a limit on the maximal width of the window. Then, we place windows in these shapes. Landscapes are placed outside the window.
Adding materials to floors, ceilings, and walls. The walls of rooms are applied with the following materials conditioned on the room type: (ceramic) square tile, concrete, brick, or plaster. The floors of rooms are applied with the following materials conditioned on the room type: tiled wood floors, square or hexagonal, alternating or non-alternating tiles, rug or concrete. The ceilings of rooms are painted with plaster. Materials are sometimes shared across different rooms.


Adding staircases. We compute the intersection of the space assigned as staircase rooms on consecutive floors. Our constraint solver will make sure that the intersection is at least the size of the staircase placeholder, which is non-empty. We randomly sample one staircase per floor (excluding the topmost one) and position them inside their corresponding staircase rooms. We reject samples where the staircase and the room in front of the steps fall outside the room or when the consecutive staircase intersects. We cut off the shape of the stairs from the room meshes and add guard rails around the stairs.
E. Constraint Solver Details
Algorithm 1 Greedy Solving Algorithm
1: procedure GreedySolver(P) 2: SimulatedAnnealing(P, Rooms, r) 3: for r in rooms do
4: SimulatedAnnealing(P, BigOb jects, r) 5: SimulatedAnnealing(P, MediumOb jects, r) 6: SimulatedAnnealing(P, S mallOb jects, r) 7: end for 8: end procedure
E.1. Greedy Solving Algorithm
Optimizing over all rooms and all objects at the same time is unfeasible due to the magnitude of the state space. As a result, we use a greedy algorithm to first solve the floor plan, then solve large, medium, and small objects, respectively. At each stage we solve each room separately. A very high-level pseudocode of our solver algorithm is given as Algorithm 1. This algorithm is not optimal in any sense, but the problem at hand is computationally intractable, and an optimal solution is not required to obtain aesthetically pleasing scenes. We provide this solver to prove our language can be optimized efficiently, and to serve as a baseline for future improvements or follow-up work.
E.2. Move Utilities
Cardinality Bounding Our solver starts with an empty scene, and must add objects during optimization to satisfy object-quantity constraints and objectives given by the user. We implement this via the Addition and Deletion moves described below, which add or remove one object. Choosing to propose a random object type with a random set of relations would have a vanishingly small likelihood of producing a move that obeys the given constraints - typically only a few object types and a few relation assignments (against wall, on floor, etc) are actually valid. To optimize efficiently, we implemented a recursive procedure to traverse through the constraint graph and find every
relevant context (such as ”on top of bookcase” or ”against livingroom wall”) available in the current scene state, retrieve any lower/upper bounds on object counts to be placed into these contexts. For example, if there are two shelves in the current state, and the user has specified each shelf shall have between 1 and 5 books placed on it, our procedure would return 2 bounds, one for each shelf, with 1 and 5 as the lower and upper bounds on object count. These bounds are sensitive to the scene’s current state: if the user specifies there should be more chairs in the dining room than tables in the dining room, then the current number of chairs will be used as an upper bound for the number of tables and vice versa. This allows optimization of arbitrary inequalities between object counts, since by randomly performing valid additions and deletions, the optimizer will explore the full space of discrete object counts for every possible context.
Degree Of Freedom Computation Moving objects in the full 6D pose space is completely unfeasible because of the plane assignment’s hard constraints that need to be satisfied. Enforcing these constraints by minimum distance scores and considering movement only on the XY plane is another option, but this still causes a violation of the hard constraints and is also wasteful as an optimizer state-space. Therefore, we calculate the degrees of freedom of each object and only move objects along the allowed subspace (e.g. painting only moves on the wall it is assigned to). For each object, we first obtain the planes that the object is constrained to move on. We then compute two types of DOFs. The translation DOFs are computed as the matrix of projection onto the intersection subspace of the planes. If the constraints are contradictory, this will be the zero matrix. The rotation DOF is either the free axis of rotation around which the object is allowed to rotate, or none if the constraints do not allow rotational movement.
Resolving Discrete Move Poses The optimizer needs to initialize every object that is added to the scene before proposing any moves to it. This initialization must obey the plane assignment hard constraints, so that the subsequent continuous moves also obey the hard constraints. Thus, we initialize objects by essentially sampling a random position on the subspace defined by the plane assignments and sampling a random rotation that is a multiple of π/2. The position sampling is done by sampling a random position on the first plane, and then repeatedly snapping the object to its assigned planes with the specified margin. The validity of the initialization is checked after each attempt, and if each initialization attempt is unsuccessful for a certain number of attempts (20 by default), the initialization is unsuccessful, and the move is reverted.


a) b) c) d) e) f)
g) h) i) j) k) l) m)
n) o) p) q) r) s) t)
u) v) w) x) y) z)
Figure 23. Variation in a chair asset with tuneable parameters. a) A base chair for comparison, followed by chairs with b) larger depth; c) thinner seats; d) wider backs; e) more curvature in seats; f) extrusion in front; g) longer legs; h) larger backs; i)-k) tilted / outward-bending / inward-bending legs; l), m) no leg bars along both axes; n) no arms; o)-p) arms with different attachments to the seat and back; q)-t) backs partially covered/supported by horizontal or vertical bars; u)-v) different leg material (woven fabric/wood); w-x) different seat material (leather/fabric); y)-z) different placement of blankets.
Reversing Moves Not every proposed move is a valid move. For instance, translating a painting too much might cause it to overhang, or reassigning a sofa to another wall might make it intersect with another object in the scene. As a result, after we apply any move, we check its validity. This is done by checking that the chosen object does not collide with any other mesh, and that all the relation constraints of the object are satisfied. If the move is not valid, then we reverse the move to remove its effects. For instance, if the object was moved by a rotation or translation, and the resulting state is not valid, we restore the backup pose of the object. If the move was an addition, we remove the object, and so on.
E.3. Move Implementations
Addition To perform an addition, we extract all available cardinality bounds from the current. Then, we discard all bounds that are tight above, i.e., those for which adding an object would violate an upper bound. The most challenging stage of addition is finding a satisfying assignment for relevant constraints. If the user specified scene[Seating].related to(room, on floor).related to(room,
against wall).count() > 0, then we know we must add some kind of seating that is against both the floor and wall. However, many options exist: Seating could mean either an armchair or a sofa, there are many possible floors to place the seating onto, and many possible wall planes attached to each floor plane. Moreover, any choice for these variables could activate additional constraints; for example, the user may have written a rule that applies to all sofas in the scene, or all objects in a particular room, so if we choose for our seating object to be a sofa, or if we choose to put it in that particular room, then additional constraints may be added to the list yet to be satisfied.
This relation assignment problem is related to classic SAT solving, except for that making an assignment can add additional terms to the equation. Alternatively, it is an SAT problem where the full equation to be satisfied is deceptively long due to new constraints being activated. We anticipate that future versions of our solver can directly incorporate classic SAT-solving approaches. However, for our current constraint programs, we have found it is sufficient to perform
exponential search over all options, visiting each child node in the search tree in a random order to ensure unbiased results. This approach is exponential in the number of se


Test Image GT Infinigen-Nature [67] Ours
Figure 24. Additional qualitative results on synthetic scenes [26].
mantic and relationship constraints involved, but fortunately, these rarely number more than 3 or 4 (IE, 1-2 semantic classes ), and the branching factor tends to be small (IE, relatively few different specific object options, or few different wall planes to assign to). For each valid assignment found, we procedurally generate a ”placeholder” asset and attempt to fit it into the scene as described in ”Resolving Discrete Moves” as described above. Placeholders are special versions of our 3D assets provided by each procedural generator which have mostly planar surfaces and lower polygon count. E.g. the placeholder for a chair would still have properly shaped legs, seat and backrest, but would not have any bevels, chamfers, nails/screws or fine geometric details. This lower polygon representation speeds up collision checking, and eliminates the need for us to heuristically detect flat planes on the object, since the asset author provides these procedurally.
Deletion Deletion uses the same cardinality bound logic as addition, but chooses a random object cardinality bound
that is not tight below, and proposes to delete it to see if the score is reduced. Typically, these moves do not help the immediate score, as we incentivize placing as many objects as possible, but they can help to eliminate particularly poorly placed objects or to avoid local optima in object counts.
Resample Resample’s primary function is to replace an existing object in the scene with an object of the same class but with new parameters. This often causes a change in shape, e.g. the length/width of a table will change, or the number of cells in a shelf may increase. Changing these parameters is desirable as it may increase/decrease the objective function (e.g. if a volume() or min distance) is changed as a result). To place the new object in the scene, we try aligning each of the bottom corners of the new bounding box with that of the old object, and check each pose for collisions, which allows the object to grow/shrink strictly to the left or right if it is attached to a wall. We assume relation assignments from the old object remain valid, since regenerating an object with new parameters does not change its semantics.


Figure 25. Additional qualitative zero-shot results on SRD [66] test dataset.
Translate Let P be the projection matrix computed as the translational degree of freedom for the chosen object. We sample ⃗x ∈ R3 where xi ∼ N(0, σ2) for i = 1, 2, 3, and the variance σ2 is proportional to the temperature. The object is then translated by P⃗x. This makes the object take a random step along the subspace on which it is constrained.
Rotate Let ⃗e be the axis of rotation computed as the rotational degree of freedom for the chosen object. We sample θ ∼ N(0, σ2) where the variance σ2 is proportional to the temperature. The object is then rotated by θ around the axis ⃗e. This makes the object take a small random rotation on the subspace on which it is constrained.
ReinitPose reinitializes the 6DOF pose of the object by resolving the discrete move poses again. Since the object relations are the same, the effect is essentially sampling a random position and orientation on the same constraint subspace. This move is useful for getting a good layout in the early stages of optimization and the cases in which an object is stuck in a sub-optimal position.
ReassignPlane . As explained in Addition, if the user specifies an object to be placed against one or more surface(s), then multiple options usually exist for which surfaces to use. This move simply attempts to swap the object to a different plane, e.g. move a sofa to a different wall, or a bottle to a different row of a shelf.
ReassignTarget Similarly, multiple options often exist for which object an object is a child of in the scene graph, e.g. a plant pot could rest on one of many different shelves/tables in a room. This move swaps the object to be a child of some other object in the scene that satisfies the same constraints as its current assignment.
F. Asset Generation Details
F.1. Asset Coverage and Variation
We provide 79 randomized procedural object generators. By category, we cover Appliances (10 generators, 112 params), Windows/Doors/Staircases (14 generators, 127 params), Furniture (17 generators, 216 params), Decorations (15 generators, 92 params), and Small Objects (19 generators, 194 params). We provide 30 material generators, 120 params total, split approximately evenly between types of wood, ceramic, fabric, metal and others. Materials are assigned to objects via customizable weighted lists, e.g. spatulas invoke either wood, plastic or metal generators for each of their ends. Following Infinigen, we report procedural parameter count as a proxy of complexity; each parameter is a random but controllable degree of freedom e.g “number of seats on a sofa”. In all, we provide 40k lines of code, of which 25k are object/material generators. For asset coverage, an incomplete list of the assets we cover is listed in Tab. 10. For asset variations, an illustration of the variation of assets is shown in Fig. 23.
F.2. Lighting and Camera Placement
Lights and windows use similar constraints as other objects (e.g. maximize count & spacing), with random wattage/temperature sampled from real-world ranges. Camera selection follows Infinigen [67]: we sample at random, reject near walls, and maximize depth variance.
G. Experiment Details
G.1. Shadow Removal
We use the model implementation from [27]’s codebase to train the two variants of the model: R (trained on real dataset only) and R+S (trained on real and synthetic datasets). Since the codebase lacked a validation set, we developed our own, comprising all image pairs across four scenes from the ISTD training dataset. Additionally, in contrast to the provided implementation, we used an L1 loss as stated in the paper. We trained the two variants for 30k steps each, including a 30epoch linear warmup phase, using AdamW [54] optimizer with default hyperparameters and a learning rate of 2e − 4. We chose the runs to have an effective batch size of 32 (by accumulating gradients for 4 steps and using the actual batch size to be 8). The training process utilized four Nvidia 3090


GPUs, with Mixed-16 precision. Fig. 25 shows additional qualitative results. We opted not to use the pre-trained model from the codebase, as our attempts to reproduce the results were unsuccessful. Nevertheless, to ensure a fair comparison, we adhered to the same implementation details for both variants. Additionally, we chose not to report SSIM (Structural Similarity Index Measure), since both models demonstrated equivalent performance, with no significant difference observed when rounding to two decimal places, for this metric.
G.2. Occlusion Boundaries
We separately train three U-Net [71] models from scratch on images generated from Infinigen Indoors, Infinigen [67] and Hypersim [70]. We apply random {cropping, brightness contrast} and color jittering with probability 0.6. We also use the RMSprop optimizer with a base learning rate of 10−5, a momentum of 0.99 and a weight decay of 10−8. Each model is trained for 10 epochs using binary cross-entropy loss. Due to the absence of ground truth occlusion boundaries in Hypersim (or any other photorealistic dataset), we approximate them by thresholding the gradient of the provided depth maps. We carefully tuned this threshold on Hypersim to give the best results. We compare the performance of the U-Net models on a curated test set of photo-realistic artist-designed synthetic 3D scenes for architecture visualization [26]. We extract the ground truth occlusion boundaries of these scenes using the tools provided in Infinigen. Additional qualitative results shown in Fig. 24 underscore our claim that the Infinigen Indoors - trained model generalizes better.
Method Mean Error
Frequency ↓
More ↑ Realistic
More Realistic Layout ↑
Realism CI 99%
Layout Realism CI 99% ProcTHOR [14] 0.252 0.107 0.056 [0.054, 0.187] [0.021, 0.127] ATISS [63] 0.232 [63] 0.287 0.307 [0.198, 0.389] [0.217, 0.410] SceneFormer [92] 0.713 [63] 0.333 0.440 [0.241, 0.439] [0.339, 0.547] FastSynth [69] 0.414 [63] 0.093 0.147 [0.046, 0.171] [0.083, 0.234] Ours 0.175 0.795 0.760 [0.750, 0.835] [0.712, 0.803]
Table 9. Perceptual Study Results. We followed the method and metrics from ATISS, but added Layout Realism, which says to only consider arrangement. We used each method’s default renderer.
H. Perceptual Study
Following ATISS [63], we conducted a perceptual study on Amazon Mechanical Turk to evaluate the realism of the generated scenes and the realism of the generated layouts. We compared Infinigen Indoors to ProcTHOR [14], ATISS [63], SceneFormer [92], and FastSynth [69]. We presented the subjects pairs of images from each method (for instance Infinigen vs ProcThor) to evaluate overall realism and layout realism. For mean error frequency, we asked the subjects if the image from a method contained any obvious errors such as flying furniture, overlapping furniture, etc. For layout realism, we asked the subjects to focus only on the arrangement
of the furniture and ignore the style of individual objects. Table 9 shows that the subjects preferred Infinigen Indoors over all methods in terms of both realism, layout realism, and the lack of obvious errors. An important caveat is that “realism” may be influenced by asset and lighting quality.


• Household appliances – Fridge, Beverage fridge (with racks) – Dishwasher (with racks) – Microwave – Oven, Stove, Oven with stove (with racks) – TV, Monitor – Kitchen Sink (with faucets) • Bathroom fixtures – Bathroom sink (standing / embedded / tabletop) – Bathtub (alcove / freestanding / corner) – Hardware (towel bar / towel ring / toilet roll paper holder / robe hooks) – Toilet (two-piece / one-piece / in-wall) • Clothes – Pants (underwear / shorts / pants) – Shirts (T-shirts / shirts) – Blankets / towel (folded / rolled) • Architectural Elements – Doors * Lite / Louver / panel / glass panel door * Door casings
– Staircases (with treads / banisters / guardrails / glass railings) * Straight / Cantilever / L-shaped / U-shaped staircase
* Spiral / curved staircase – Rugs – Warehouse racks / pallets • Seatings – Bar stool / office chair – Armchair / dining chair / side chair / spholstered chair – Beds (bedframe / mattress / pillow) – Sofa • Shelves (with drawers and doors) – Cabinets / kitchen cabinets – Cell shelves / wall shelves / bookcases / triangle shelves • Table decorations – Books (column / stack) – Vases / Aquarium tank – Plants in pots (floor-top / table-top) • Tables – Desks / Cocktail table / Dining table / Kitchen table • Tableware – Bottle (soda / wine / beer / juice) / jar – Chopsticks / Knife(table knife / cleaver / chef’s knife) / forks / spoons / spatulas / bowl / plate – Cup (mug / shot glass / teacup / plastic cup) / wineglass – Food bag(chip bag / food pouch / food bar) / Food box / can / jar / Fruits in containers(i.e. tableware with fruits placed inside) – Pan / pot(Cooking pot / saucepan) / lid(of pots and pans) • Wall decorations – Balloons / wall arts / mirror • Windows – Sliding / awning / casement / glassblock / bay window
Table 10. Coverage of the assets in Infinigen Indoors.