DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation
Xiaoliang Ju1,* Zhaoyang Huang1,* Yijin Li2, Guofeng Zhang2, Yu Qiao3, Hongsheng Li1 1 MMLab, The Chinese University of Hong Kong 2 Zhejiang University 3 Shanghai AI Laboratory
{akira,drinkingcoder}@link.cuhk.edu.hk, hsli@ee.cuhk.edu.hk
Stage 1 Stage 2 Stage 3
Pure Noise Volume Rough Occupancy Delicate Occupancy Mesh from TSDF
Texture added by DreamSpace Indoor Scene Geometry Generated by DiffInDScene Texture added by DreamSpace
Figure 1. Coarse-to-fine indoor scene geometry generation using a sparse diffusion framework. For better visualization, the texture is produced by DreamSpace [51] after the scene geometry is generated by our DiffInDScene.
Abstract
We present DiffInDScene, a novel framework for tackling the problem of high-quality 3D indoor scene generation, which is challenging due to the complexity and diversity of the indoor scene geometry. Although diffusionbased generative models have previously demonstrated impressive performance in image generation and object-level 3D generation, they have not yet been applied to roomlevel 3D generation due to their computationally intensive costs. In DiffInDScene, we propose a cascaded 3D diffusion pipeline that is efficient and possesses strong generative performance for Truncated Signed Distance Function (TSDF). The whole pipeline is designed to run on a sparse occupancy space in a coarse-to-fine fashion. Inspired by KinectFusion’s incremental alignment and fusion of local TSDF volumes, we propose a diffusion-based SDF fusion
∗ Joint first authorship
approach that iteratively diffuses and fuses local TSDF volumes, facilitating the generation of an entire room environment. The generated results demonstrate that our work is capable to achieve high-quality room generation directly in three-dimensional space, starting from scratch. In addition to the scene generation, the final part of DiffInDScene can be used as a post-processing module to refine the 3D reconstruction results from multi-view stereo. According to the user study, the mesh quality generated by our DiffInDScene can even outperform the ground truth mesh provided by ScanNet. Please visit our project page for the latest progress and demonstrations: https://akirahero. github.io/diffindscene/.
1. Introduction
3D scene production is a fundamental task in 3D computer vision with many applications, such as Augmented Reality (AR), game development and embodied AI [31], where the quality of 3D scene geometry plays a paramount role.
1
arXiv:2306.00519v4 [cs.CV] 29 Nov 2023


While the 3D reconstruction from multi-view stereo [2, 45] can recover scenes from real-world, the quality of the resultant meshes is far from satisfactory, for the major mesh details might be lost during iterative fusion. Recently, diffusion models [13, 43] have shown their great ability in generating images and objects of high quality. Here we want to ask a question: Can we exploit diffusion models to produce 3D scenes? In this paper, we propose a novel framework DiffInDScene, which not only helps optimize the results of 3D reconstruction, but also generates high-quality indoor spatial geometry from scratch (see Fig. 1).
Diffusion models are a class of generative models designed for synthesizing data by iterative denoising. The popular Denoising Diffusion Probabilistic Model (DDPM) training paradigm starts the denoising from pure Gaussian noise. Training diffusion models for room-level Truncated Signed Distance Function (TSDF) volumes is challenging because of its large size. Previous 3D diffusion models only focus on object-level 3D generations. As reported by InstantNGP [27], only 2.57% voxels are informative in common 3D scenes.
To deal with the large scale of indoor scenes, we propose a coarse-to-fine sparse diffusion pipeline consists of multiple stages. The first few stages are used to generate the occupancy volume in a coarse-to-fine manner, and the last one generates the TSDF values in the sparsely occupied voxels. For the first few occupancy generation stages, a multi-scale auto-encoder is designed to encode occupancy to latent space, providing feature guidance for occupancy generation. This approach allows us to employ latent diffusion and further compress the size of input volumes. Additionally, we propose a sparse 3D diffusion model denoising only on the sparsely occupied voxels of TSDF or the occupancy latent volumes, which saves two orders of computational and memory costs.
Although the cascaded sparse diffusion pipeline significantly reduces the required computational resources, the considerable variation in room sizes still poses a challenge when attempting to directly train on a room-level TSDF volume, at the final stage of our cascaded diffusion process. To increase the data variation within each mini-batch, we randomly crop local TSDF volumes of smaller sizes from the original large volume for training. During inference, we design a stochastic TSDF fusion algorithm that generates the entire room by iterative denoising and fusing local TSDF volumes. The fusion method allows for the generation of a complete and unified TSDF within a large scene while efficiently decomposing the scene into smaller crops, thereby conserving computing resources. Our proposed diffusion method can be also utilized to refine indoor scene meshes such as the reconstruction results from multi-view stereo, such as NeuralRecon [45]. Given the TSDF volume of reconstructed scene as occupancy condition, our diffu
sion model can effectively refine and optimize the TSDF volume towards the ground truth. Our contributions can be summarized as four-fold: 1) We propose a novel framework DiffInDScene for room-level indoor scene generation with a sparse diffusion model that saves two orders of resource consumption. 2) We design a multi-scale auto-encoder to provide feature guidance for the scene occupancy generation. 3) We propose a novel algorithm that fuses diffusion-based local TSDF volumes, which enables large-scale indoor scene generation. 4) DiffInDScene exhibits a promising capability in producing highquality room-level geometry through both generation from scratch and refinement of existing reconstructions.
2. Related Works
Diffusion Models. The diffusion model [13, 42, 43] has emerged as a promising class of generative models for learning data distributions through an iterative denoising process. They have shown impressive visual quality in diverse applications of 2D image synthesis, encompassing image inpainting [24], super-resolution [15, 39], editing [26], text-to-image synthesis [28, 36], and video generation [14, 16]. Nevertheless, the application of diffusion models in the 3D domain has received limited attention in comparison to the extensive exploration seen in the 2D domain. In the 3D domain, existing research has focused on the generation of individual objects [18, 25, 30, 53], while less attention has been paid to the synthesis of entire scenes, which possess significantly higher levels of semantic and geometric complexity, as well as the expansive spatial extent present in 3D scene synthesis. 3D Shape Generation. Extensive exploration has been conducted on various 3D representation methods, such as voxel [48, 50], point cloud [29, 49, 52], and implicit field [22, 41], in conjunction with different generation models such as diffusion models [41] and GANs [48]. While these approaches have shown success in object-level generation, transferring them to scene generation at a larger scale is still challenging. Firstly, the large scale leads to exponential growth in computation resources consumption in training and inference processes. Additionally, object-level generation is comparatively easier due to simpler geometries and less diversity. 3D Scene Synthesis. In recent decades, the field of 3D scene synthesis has experienced extensive investigation, particularly driven by the proliferation of 3D indoor scene datasets [5, 8] and advancements in 3D deep learning [23, 32, 33]. However, current methods mainly focus on synthesizing plausible 3D scene arrangements [4, 10, 12, 34]. They usually learn to synthesize the scene graph as the intermediate scene representation and retrieve objects from available dataset. In contrast to these methods, we aim to simultaneously synthesize both the scene arrangement and
2


the detailed geometry. Text2Room [17] is the most related work to ours in recent years, which leverages pre-trained 2D text-to-image models to synthesize a sequence of images and then conduct an iterative reconstruction. While such method can produce room-scale geometry, the results are often fragmentary and distorted, limiting their practical applications in areas such as gaming or AR.
3. Methodology
3.1. Overview
To generate room-level 3D geometry, the greatest challenge is the large scale, as it requires substantial computing resources. We employ a cascaded diffusion model to generate the whole room in a coarse-to-fine manner. The first stage is to generate the coarse structure of the whole room. The following stages further refine the rough shape to a 3D occupancy field with higher resolutions. At the final stage, the resolution increases to the highest level, and we crop the whole scene to overlapped pieces to generate the final detailed Truncated Signed Distance Function (TSDF) volume. In every stage, we use a separate sparse diffusion model to reduce the resource consumption, which exclusively denoises on sparsely distributed occupancy. In our implementation, we use 3 stages to create indoor geometry up to size of 512 × 512 × 128.
Such cascaded solution has three advantages. First, the computation resource consumption is constrained in all stages. Second, compared with piece-wise generation or incremental generation methods, the first stage of our model is able to sketch the global structure of the scene, which helps to generate a complete layout with unified and detailed structure. Third, every stage can be trained independently, and the generation process can be stopped in advance when generating an unsatisfied layout at early stage.
3.2. Cascaded Diffusion for Indoor Geometry Generation
We propose a sparse cascaded diffusion model as shown in Fig. 2 (a). In our implementation, a 3-stage diffusion process is utilized to generate a complete indoor scene starting from noise. The first 2 stages are utilized to generate and refine the 3D binary scene occupancy volume, and the final is used to generate TSDF value within the occupancy. As TSDF volume only retains information near the object surface, we simply define all voxels containing valid TSDF values as the binary scene occupancy. Assume we have multi-scale occupancy embeddings z(1), z(2) of a TSDF volume x with increasing resolutions, and their binary occupancy masks are Mz(1) , Mz(2) , Mx, satisfying
Mz(2) = G1 z(1), Mz(1) , (1)
Mx = G2 z(1), z(2), Mz(2) , (2)
where G1, G2 are occupancy decoders, and we will explain them in detail in Section 3.3 together with the occupancy latents z(1), z(2). Then the 3-stage diffusion processes {D1, D2, D3} can be established as follows.
• Stage 1 generates the occupancy latent code z(1) of the lowest resolution. Given a fixed volume z(1)
T and its oc
cupancy mask Mz(1)
T
, the diffusion process performs de
noising operation in T timesteps to obtain z(1)
0 as
z(1)
0 = D1(z(1)
T , Mz(1)
T
), (3)
where z(1)
T is filled with Gaussian noise. By defining
mask Mz(1)
T
according to datasamples, we can include
data samples with varying volume sizes in each minibatch without padding or cropping, and control over the maximum area for scene generation. • Stage 2 generates latent code of higher resolution z(2) conditioned on z(1) as
z(2)
0 = D2(z(2)
T , Mz(2)
T
; z(1)
0 ), (4)
where Mz(2)
T
is obtained utilizing Eq. (1), and z(2)
T is filled
with another Gaussian noise volume. • Stage 3 generates the final TSDF volume with all those generated latent codes z(1), z(2) as input conditions
x0 = D3(xT , MxT ; z(1)
0 , z(2)
0 ), (5)
where MxT is obtained by Eq. (2), and MxT is filled with Gaussian noise.
Compared with generating occupancy directly, the occupancy embedding can guide the refinement of the occupancy throughout the diffusion process, particularly at the initial stage of generation. Sparse Diffusion. We follow DDPM [13, 42] to implement the sparse diffusion. Each stage of our model employs a separate sparse diffusion, with the only distinction lies in the types of input and output. In this section, we use v to represent any kind of volumes such as TSDF x and latent codes z(1), z(2), and y to denote the diffusion condition with M as their shared sparsity mask. DDPM [13, 42] transforms a sample volume v0, to a white Gaussian noise vT ∼ N (0, 1) in T steps. In each step t, the sample vt is obtained by adding i.i.d. Gaussian noise with variance βt and scaling the sample in the previous step
vt−1 with √1 − βt:
q (vt | vt−1) = N vt; p1 − βtvt−1, βtI , (6)
3


(a) Sparse Cascaded Diffusion Pipeline (3-stage Implementation)
Sparse DM
Sparse
DM Occupancy Decoder
Up-sampler
Occupancy Decoder
Up-sampler
Up-sampler
Sparse DM
Volume with Arbitrary Size
Stage 1 Stage 2 Stage 3
z(")
z($)
Encode Quantize Decode
Encode Quantize
Decode
Up-sample
TSDF Volume Crop
Latent Code
(b) Learning the Latent Space for Multi-scale Occupancy
Latent Code
z(")
z($)
zq
(")
zq
($)
M!(") z(#)
, up-sampled
M% z(&)
, up-sampled
D" D$ D&
Sparsely Reconstructed TSDF Volume Crop
Sparse TSDF Volume
Figure 2. Sparse cascaded diffusion with a multi-scale occupancy embedding.
which is also called the forward direction. On the other hand, the reverse process can be depicted as:
pθ (vt−1 | vt, y, M ) = N (vt−1; μθ, Σθ) , (7)
where y denotes the extra condition. Diffusion models are trained to reverse the forward process, predicting μθ as
μθ (vt, y, M, t) = √1αt
vt − βt
√1 − α ̄t
εθ (vt, y, M, t) ,
(8) where αt := 1 − βt, α ̄t := Qt
s=0 αs, and εθ is the neural network with the parameter set θ, which has a UNet-like structure. With the occupancy mask M , εθ denoises only sparsely occupied voxels with sparse convolutions and attentions. We implement εθ using the engine of TorchSparse [46]. The details of network architecture are provided in the supplementary materials. A mean square error loss masked by M is used to supervise the noise prediction as
Ldiff = Et,v0,ε,y,M M ⊙
h
∥ε − εθ (vt, y, M, t)∥2
2
i
.
(9)
3.3. Learning the Latent Space for Multi-scale Occupancy
To obtain hierarchical occupancy embeddings and their decoders, we design a multi-scale Patch-VQGAN as Fig. 2(b), inspired by VQ-VAE-2 [35]. The encoder takes TSDF volume x as input, and outputs the occupancy embedding z(1), z(2). These embeddings are expected to be decoded to the occupancy mask Mz(2) and Mx as Eq. (1) and Eq. (2),
where ground truth of Mz(2) is obtained by downsampling from Mx via maxpooling. To capture more shape details, we also add a TSDF decoding head. After training, the encoder is no longer utilized, and only the occupancy decoders are employed to convert the latent code to occupancy.
Formally, we define the encoder as E, the decoders as G, and the element-wise quantization of latent volume as q(·). Any ground truth TSDF volume x ∈ RH×W ×L can be encoded progressively into (z(1), z(2)) = E(x), where z(1), z(2) ⊆ Rd. The quantization is formulated as
q(z) := (arg min
zp∈Z ∥zijk − zp∥) ∈ Rh×w×l×d, (10)
where Z = {zk}K
k=1 ⊂ Rd denotes the discrete codebook of size K, and zijk represents the latent vector at coordinate
i, j, k of the latent volume. For simplicity, we define z(1)
q := q(z(1)), z(2)
q := q(z(2)) in Fig. 2(b).
During the training process, we randomly crop cubes of 96 × 96 × 96 from the original TSDF volumes as data samples, so that diverse crops from different scenes are contained in each mini-batch. The training loss is defined in Eq. (11), where Lrec, Lvq, LGAN denote the reconstruction loss, vector quantization loss and the adversarial loss from a simple multi-layer discriminator,
L = Lrec + λ1Lvq + λ2LGAN, (11)
where λ1 and λ2 are hyper parameters to weight losses of different types. For the encoder-decoder training, we use a L1 loss to supervise the TSDF value, and binary crossentropy (BCE) to supervise the occupancy masks, as shown
4


0.1 0.0 0.2 0.7 0.2 0.5 -0.6 0.4 0.4 -0.1 0.2 0.2 0.1 0.9 -0.1 0.7
0.2 -0.2 0.7 0.5 0.3 0.4 -0.6 0.1 0.3 -0.1 0.2 0.6 0.0 0.8 -0.1 0.9
Arbitrary
S
liding
D
irection
0.1 0.0 0.2 0.7
0.2 0.5 -0.6 0.4
0.4 -0.1 0.2 0.2
0.1 0.9 -0.1 0.7
-0.6 0.7 0.5
0.3 -0.6 0.1
0.2 0.6
0.0 0.8 -0.1 0.9
0.3
4
-0.2
Noise Prediction at Same Timestep
Diffusion Inference in Overlapped Sliding Windows
Stochastic Local Fusion
Large Scene
Figure 3. Stochastic TSDF Fusion.
in Eq. (12).
Lrec = BCE(Mˆx, Mx) + BCE(Mzˆ(2) , Mz(2) ) + ∥x − xˆ∥1 (12) The Lvq, LGAN are defined similar to [9].
3.4. Local Fusion for Global Diffusion
The volume cropping operation in the training of the final stage diffusion raises a question: how to infer on a complete scene using a model trained on crops? The independent generation in a crop-by-crop manner as image generators such as [24, 36] may cause inconsistent results between adjacent crops. To address this issue, a fusion algorithm is proposed to perform the joint diffusion process concurrently on the overlapping local volumes. During inference, we split the indoor space into K overlapping 3D crops {P0, P1, . . . , PK−1, } that cover the entire room, and generate the TSDF for the entire room by concurrently diffusing the K crops with stochastic fusion. We denote the global TSDF at the timestep t as xt and the k-th crop at the timestep t as xtk and the global TSDF at the timestep t as xt. At time step t, we need to obtain the global TSDF xt by fusing local TSDFs xtk and then up
date local TSDFs from the global TSDF: xtk(pi) = xt(pi). After synchronizing local TSDFs with fusion, each crop step to the next time step individually. Specifically, for a voxel grid p, suppose G(p) contains the crops that cover p: G(p) := {k|p ∈ Pk}, we need to obtain xt(p) by fusing the crops {xtk(p)|k ∈ G(p)} overlapping on p. Stochastic TSDF Fusion. A straightforward fusion algorithm is taking the average TSDFs of the local crops: xt(p) = 1
|G(p)|
P
k∈G xtk(p), which is also adopted by the classical KinectFusion [19]. However, average fusion significantly reduces the variance of the sample distribution. To ensure the generation quality and global consistency, we propose stochastic fusion to keep the distribution and fuse TSDFs in the reverse process. An example of 2-windows fusion is shown in Fig. 3. Specifically, we randomly sample an index k from G(p) in a uniform distribution to update the global TSDF xt(p) = xtk(p), which remains the distri
Optional Condition (e.g. SDF value)
MVS Sparse
DM
Local Fusion
Rough Occupancy Crops Complete Scene
Figure 4. Refine or recover scene from MVS methods.
bution:
xt(p) ∼ N (μk
t (p), Σk
t (p)), k = RandomSelect (G(p)) . (13)
3.5. Extension to Refining 3D Reconstruction
Given a rough geometry represented by the occupancy volume of a scene, the TSDF volume can be generated by directly adopting the final stage of our cascaded diffusion framework.
The most direct application is to recover/refine scenes produced by multi-view stereo methods or even LiDAR mappers as depicted in Fig. 4, as obtaining an occupancy of the scene is relatively straightforward using Multi-view Stereo (MVS) techniques. If the MVS method provides a TSDF result, it can be utilized as the diffusion condition to our cascaded diffusion network. The diffusion process is applied to the crops of the occupancy volume as inputs, and the results are fused using our local fusion module at each timestep, so that we can obtain a complete and refined scene reconstruction.
4. Experiment
DiffInDScene is a versatile tool capable of generating detailed indoor scene geometry at the room level. It is not only capable of creating scenes from scratch but also has the ability to refine or recover scenes using occupancy fields produced by multi-view stereo methods. To train DiffInDScene for scene generation, we utilized the 3DFRONT dataset [11] provided by Alibaba, which consists of 6813 furnished houses with meshes. For training purposes, we selected 5913 houses with dimensions smaller than 512 × 512 × 128, with a voxel size of 0.04m. For scene refinement or recovery on multi-view stereo (MVS), we trained DiffInDScene using the training split of the ScanNet dataset, with the NeuralRecon [45] as the MVS module. The ScanNet dataset contains 1613 indoor scenes, each accompanied by ground-truth camera poses and surface reconstructions. To ensure consistency, we adopted the same data split as NeuralRecon for training and evaluation purposes.
5


Table 1. Geometry quality comparison, including mesh quality (Aspect Ratio, Circularity, and Shape Regularity) and user study on the completeness and perceptual quality.
Text2Room [17] Text2Room
+ Poisson [21] Ours Aspe. mean↑ 0.416 0.443 0.473 Aspe. var↓ 0.022 0.029 0.009 Circ. mean↑ 0.674 0.709 0.781 Circ. var↓ 0.052 0.057 0.022 Shap. mean↑ 0.716 0.730 0.816 Shap. var↓ 0.045 0.060 0.023
Completeness 2.532 3.228 4.856 Perceptual 2.472 2.812 4.836
4.1. Indoor 3D Scene Generation
In this section, we will present the results of our indoor scene generation, both quantitatively and qualitatively. To the best of our knowledge, Text2Room stands as the stateof-the-art solution to generate 3D geometry of room-level indoor scenes. Since there are few other works that can directly generate scene structures and output the corresponding mesh models, here we also compare with ”Text2Room + Poisson” to enrich the experimental comparison, which means a Poisson reconstruction [21] is added as a refinement of Text2Room. Metrics. We employ mesh quality as a measure to reflect the overall quality of the generated 3D scene geometry. Regarding mesh quality, we noticed that noisy meshes and high-quality meshes exhibit distinct distributions of triangle shapes. Noisy meshes and problematic regions are characterized by triangles with low aspect ratio, circularity, and shape regularity, as introduced in [3, 7]. Therefore, these 3 factors are selected as the objective metric of mesh quality. In addition, we perform a user study similar to the one conducted in Text2Room [17] as the subjective metric on the mesh quality, including the completeness and perceptual quality. The scores range from 1 to 5, with higher scores indicating a better alignment with the evaluation metrics. Quantitative Results. For the objective metrics, We randomly select 11 scenes from different methods, and calculate the average value of all triangular mesh faces. For the user study, we randomly choose 5 scenes generated by each method and gather ratings from a group of 40 users with basic knowledge in 3D modeling. We summarize the evaluation results as Table 1 shows. Our method demonstrates superior performance on those metrics compared to the the other approaches. While employing Poisson reconstruction as a refinement step enhances the results of Text2Room, it does not lead to a significant improvement. Qualitative Results. The generated scene samples from different methods are listed in Fig. 5. We compare both un
Table 2. Resource Consumption Comparison.
TFLOPs Parameters(M) GPU Memory(GB) Batch Size 1 1 1 2 4 Sparse 0.008 161.5 11.8 15.3 22.8 Dense 3.290 161.5 22.8 - 
textured meshes and textured scene renderings in our evaluation. While Text2Room is capable of reconstructing the scene from images, it often results in serious distortions and fragmentation. As indicated by our quantitative evaluation, applying Poisson reconstruction helps in filling the holes, but it has limited impact on improving the overall structure of the scene. Our method can produce larger rooms with clearer structures and more complex layout. Furthermore, by incorporating DreamSpace as a post-processing step, we can achieve high-quality scene renderings. This combination allows for enhanced visual output and improved overall scene representation. Compared with the middle-size house showed in Fig.5, two larger and more complex samples are shown in Fig. 6. Additional samples showcasing larger views can be found in the supplementary materials.
4.2. Ablation Study
Sparse or Dense. To compare the dense diffusion and the sparse diffusion, we implement two networks using sparse and dense convolution respectively, with exactly the same structures. Several randomly cropped TSDF volumes (96 × 96 × 96) from ScanNet dataset are fed into these two models. The resource consumption of the two strategies are shown in Table 2. This experiment is conducted on the platform equipped with RTX3090 GPU with 24GB memory, with the diffusion model running in training status. The sparse diffusion requires fewer resources and runs faster with fewer parameters, due to the characteristic of the occupancy distribution. In our test data, the largest occupancy rate of the TSDF crops are less than 20%. Diffusion with Fusion. To compare the different fusion methods mentioned in Section 3.4, we perform the final stage of our cascaded diffusion model on the given occupancy from a scene of ScanNet dataset. The results are shown in Fig. 7. Individual diffusion presents significant inconsistency between adjacent crops. Average fusion generates meshes of lower quality because the sample distribution during diffusion is disturbed. Our stochastic diffusion remains global consistency and generates high-quality meshes.
4.3. Refinement and Recovery on MVS
Metrics. Our objective is to obtain high-quality 3D scenes from MVS, aiming for results comparable to the ground truth in various aspects, including details, completeness,
6


Ours Text2Room + Poisson Text2Room
Geometry Texture Rendering Geometry Texture Rendering
Ours
Figure 5. Comparison of indoor scene generation. The texture renderings of our results are produced by DreamSpace [51]. The ceilings are removed from the original meshes for a better visualization.
Figure 6. Samples with more complex structures generated by DiffInDScene. Note that the floating objects in this figure are ceiling lamps, which are reserved after we remove the ceilings for visualization.
tightness, and sharpness. As a generative model, the proposed method operates within a relaxed occupancy, which
may cause drift from the ground truth. As a result, conventional correspondence-based metrics for 3D reconstruction are not suitable. Instead, we employ metrics unaffected by the relaxed occupancy, such as normal error distribution and mesh quality.
Normal error is used to evaluate the similarity of surface orientation between the reconstructed and ground truth meshes. Smaller normal errors indicate better alignment to the ground truth. We filter out outliers with errors larger than 90◦ and analyze the percentage of inlier normals below a threshold (< T ◦ ratio) and the mean normal error of those inliers. We use the same mesh quality metrics as Section 4.1.
Results. The samples of scene reconstruction are shown in Fig. 8. As Table 3 shows, our method significantly outperforms the other three methods on all thresholds of nor
7


Individual Average Stochastic
Figure 7. Comparison of different fusion methods in the final stage of our cascaded diffusion.
Table 3. Normal error comparison. “NR”, “Lap”, and “SR” are the abbreviations of NeuralRecon, Laplacian and SimpleRecon, respectively.
NR [45] NR + Lap [44] SR [40] NR + Ours
< 90◦mean↓ 34.65 37.12 35.44 30.4 < 90◦ ratio↑ 100% 100% 100% 100% < 45◦ mean↓ 10.27 12.34 12.51 8.09 < 45◦ ratio↑ 59.97% 57.97% 60.20% 65.05% < 30◦ mean↓ 6.45 8.17 8.31 5.05 < 30◦ ratio↑ 52.88% 49.89% 51.67% 59.27%
mal error. As for the mesh quality, we compute the mean and variance of the three scores described above and compare them in Table 4. Our results presents significantly better performance than the other three methods, i.e., higher scores with smaller score variance. Moreover, we also include ground truth mesh for comparison. Interestingly, our results also outperforms the ground truth except for the aspect ratio score, which indicates the high quality of our reconstructed meshes. User Study. The user study is conducted to rank scene quality, providing subjective evaluations to complement the objective metrics. We randomly select 10 scenes from the test split of the ScanNet dataset and generate reconstructed meshes using four methods. Users are asked to rank the methods based on details, completeness, plane quality, and edge quality. We collect ranking results from 51 users. As shown in Table 5, our reconstructed mesh significantly outperforms NeuralRecon, “NeuralRecon+Laplacian Denoising” (denoted as ”NR + Lap”), and even surpasses the qual
(a) Scene samples refined on NeuralRecon (NR).
(b) Comparison of scene quality from different methods.
NR + Ours
NR Ground Truth
Figure 8. Sample scene reconstructions on ScanNet dataset. The meshes are colored according to curvatures in sub-figure(b), where green regions denote lower curvatures.
Table 4. Mesh quality comparison. We compare the mean and variance of three scores: Aspect Ratio, Circularity, and Shape Regularity. NR(occ) means only the occupancy is used, without the conditional TSDF depicted by the dash line in Fig. 4.
NR [45] NR +
Lap [44] SR [40] NR(occ)
+ Ours
NR +
Ours GT Aspe. mean↑ 0.459 0.437 0.436 0.469 0.457 0.477 Aspe. var↓ 0.022 0.023 0.024 0.020 0.016 0.022 Circ. mean↑ 0.740 0.712 0.708 0.742 0.763 0.758 Circ. var↓ 0.041 0.052 0.054 0.041 0.030 0.034 Shap. mean↑ 0.772 0.746 0.739 0.793 0.797 0.793 Shap. var↓ 0.041 0.052 0.055 0.041 0.030 0.031
ity of the ground truth meshes.
5. Conclusion
We have presented DiffInDScene as a novel framework for diffusion-based high-quality indoor scene generation. DiffInDScene mainly consists of three modules: 1) a sparse diffusion network that efficiently denoises 3D volumes on occupied voxels, 2) a multi-scale Patch-VQGAN for occu
8


Table 5. User study on the refinement of scene reconstruction.
NR [45] NR + Lap [44] NR + Ours GT Details↑ 12.26 6.80 17.41 25.50 Completeness↑ 11.15 8.84 21.30 20.76 Tight Plane↑ 5.48 12.10 25.85 18.17 Sharp Edge↑ 8.22 10.20 22.58 21.19 Overall (Sum)↑ 37.10 37.98 87.14 85.62
pancy decoding, 3) a cascaded diffusion framework to generate room-level scene from scratch, and 4) a stochastic fusion algorithm for diffusion-based local TSDFs, which enables large-scale indoor scene generation. In the future, we will explore to generate scenes with various conditions such as text and sketch.
References
[1] Christopher Batty. Signed distance field generator for triangle meshes, 2015. 1 [2] Aljaz Bozic, Pablo Palafox, Justus Thies, Angela Dai, and Matthias Nießner. Transformerfusion: Monocular rgb scene reconstruction using transformers. Advances in Neural Information Processing Systems, 34:1403–1414, 2021. 2
[3] Jan Brandts, Sergey Korotov, and Michal Kˇr ́ıˇzek. On the equivalence of regularity criteria for triangular and tetrahedral finite element partitions. Computers & Mathematics with Applications, 55(10):2227–2233, 2008. Advanced Numerical Algorithms for Large-Scale Computations. 6 [4] Angel Chang, Manolis Savva, and Christopher D Manning. Learning spatial knowledge for text to 3d scene generation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 2028–2038, 2014. 2 [5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. 2
[6] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020. 1
[7] Paolo Cignoni, Marco Callieri, Massimiliano Corsini, Matteo Dellepiane, Fabio Ganovelli, Guido Ranzuglia, et al. Meshlab: an open-source mesh processing tool. In Eurographics Italian chapter conference, pages 129–136. Salerno, Italy, 2008. 6 [8] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828–5839, 2017. 2 [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873–12883, 2021. 5, 1
[10] Matthew Fisher, Manolis Savva, Yangyan Li, Pat Hanrahan, and Matthias Nießner. Activity-centric scene synthesis for functional 3d scene modeling. ACM Transactions on Graphics (TOG), 34(6):1–13, 2015. 2 [11] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10933–10942, 2021. 5, 1 [12] Qiang Fu, Xiaowu Chen, Xiaotian Wang, Sijia Wen, Bin Zhou, and Hongbo Fu. Adaptive synthesis of indoor scenes via activity-associated object relation graphs. ACM Transactions on Graphics (TOG), 36(6):1–13, 2017. 2
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. 2, 3 [14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2
[15] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23(47):1–33, 2022. 2 [16] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. 2 [17] Lukas Ho ̈llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989, 2023. 3, 6, 2, 4
[18] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3d shape generation. In SIGGRAPH Asia 2022 Conference Papers, pages 1–9, 2022. 2 [19] Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, et al. Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages 559–568, 2011. 5, 1 [20] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. 1
[21] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, page 0, 2006. 6, 3, 4 [22] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusionsdf: Text-to-shape via voxelized diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12642–12651, 2023. 2 [23] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Pointvoxel cnn for efficient 3d deep learning. Advances in Neural Information Processing Systems, 32, 2019. 2
9


[24] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11461–11471, 2022. 2, 5 [25] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2837–2845, 2021. 2 [26] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2021. 2 [27] Thomas Mu ̈ller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1–15, 2022. 2 [28] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2
[29] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 2
[30] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2
[31] Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, et al. Habitat 3.0: A co-habitat for humans, avatars and robots. arXiv preprint arXiv:2310.13724, 2023. 1
[32] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652–660, 2017. 2 [33] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017. 2
[34] Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and Song-Chun Zhu. Human-centric indoor scene synthesis using stochastic grammar. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5899–5908, 2018. 2 [35] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 4
[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ̈orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022. 2, 5
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo ̈rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 1 [38] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 110, 2022. 1 [39] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image superresolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 2
[40] Mohamed Sayed, John Gibson, Jamie Watson, Victor Prisacariu, Michael Firman, and Cl ́ement Godard. Simplerecon: 3d reconstruction without 3d convolutions. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII, pages 1–19. Springer, 2022. 8 [41] Jaehyeok Shim, Changwoo Kang, and Kyungdon Joo. Diffusion-based signed distance fields for 3d shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20887–20897, 2023. 2 [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR, 2015. 2, 3 [43] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438–12448, 2020. 2 [44] Olga Sorkine, Daniel Cohen-Or, Yaron Lipman, Marc Alexa, Christian Ro ̈ssl, and H-P Seidel. Laplacian surface editing. In Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, pages 175–184, 2004. 8, 9 [45] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598–15607, 2021. 2, 5, 8, 9 [46] Haotian Tang, Zhijian Liu, Xiuyu Li, Yujun Lin, and Song Han. Torchsparse: Efficient point cloud inference engine. Proceedings of Machine Learning and Systems, 4:302–315, 2022. 4, 1 [47] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https : / / github . com / huggingface / diffusers, 2022. 1
[48] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural information processing systems, 29, 2016. 2
10


[49] Zijie Wu, Yaonan Wang, Mingtao Feng, He Xie, and Ajmal Mian. Sketch and text guided diffusion model for colored point cloud generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 89298939, 2023. 2 [50] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8629–8638, 2018. 2 [51] Bangbang Yang, Wenqi Dong, Lin Ma, Wenbo Hu, Xiao Liu, Zhaopeng Cui, and Yuewen Ma. Dreamspace: Dreaming your room space with text-driven panoramic texture propagation. arXiv preprint arXiv:2310.13119, 2023. 1, 7, 4
[52] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4541–4550, 2019. 2 [53] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5826–5835, 2021. 2
11


DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation
Supplementary Material
6. Video Demonstration
To gain a more comprehensive understanding of our method for generating the indoor scene, we kindly invite you to watch the attached video. The video demonstrates an example of the coarse-to-fine generation process, and the the post-processing of texturing using DreamSpace [51]. Furthermore, to provide a more detailed and complete perspective on the inner scene structures, a random walk is conducted within the generated scene.
7. Implementation Details
7.1. Dataset and Preprocessing
Indoor Scene Generation from Scratch. 3D-FRONT [11] provides professionally designed layouts and a large number of rooms populated by high-quality 3D models. However, when organizing the mesh models to a complete scene, the meshes may intersect with each other. Additionally, most of them are not watertight meshes. These factors lead to erroneous Truncated Signed Distance Function (TSDF) volumes. In such cases, the meshes retrieved from TSDF volumes contains lots of wrong connections. To address this problem, we perform a solidification and voxel remeshing on each scene mesh, using a pipeline of modifiers from Blender with a voxel size of 0.02m. All meshes are saved as triangular format. After the watertight meshes are obtained, we derive the SDF volumes by using a open-source software SDFGen [1], with a resolution of 0.04m. Then the SDF volumes are truncated to TSDF by a maximum distance of 0.12m.
Refinement on the Reconstruction from Multi-view Stereo(MVS). We use the official train / validation / test split of ScanNet(v2) dataset, including 1201 / 312 / 100 scenes respectively. For there is no TSDF ground truth provided in this dataset, we adopt a TSDF fusion method like [19] to produce the ground truth as NeuralRecon does. We only use TSDF data without any other data type such as images in the whole training/testing process. To compare the reconstruction results with pretrained NeuralRecon, the grid size of TSDF volume is set to 0.04m, and the truncation distance is set to 0.12m. The default value of the TSDF volume is 1.0. In the training process, a random volume crop of 96 × 96 × 96 is used as data augmentation, where a random rotation between [0, 2π] and a random translation is performed before cropping. To ensure that the sampling crop contains sufficient occupied voxels, the translation is limited in the bounding box of global occupied region, and the en
tire cropped volume should be within the boundary of this region.
7.2. Sparse Diffusion Model
Network Structure. TorchSparse [46] is used to implement the UNet structure of our network for noise prediction. A group normalization(32 groups) and a SiLU activation are used successively before any layer of sparse convolution. The network strctures used in difference stages of our cascacded diffusion are shown in Fig. 9, where SparseRes and Spatial Transformer are key components of our implementation as shown in Fig. 10.
Training & Inference Settings. The network parameters are randomly initialized in training process, and we use the Adam optimizer with a learning rate of 1.0 × 10−4. As for the diffusion framework, the DDIMScheduler in the open-source diffusers [47] is developed as our codebase. Following [6] and [38], we adopt the α−conditioning to stabilize training, and enable the parameter tuning over the noise schedule and the timesteps during inference stage. More concretely, the cumulative product of αt namely α ̄t is used as a substitute of the timestep t as time embedding in most existing works. In Section 4.1, we use a cosine noise schedule with 2000 timesteps during training, and the same noise schedule is used with 200 time-steps during inference within the DDIM framework. In Section 4.3, we use a linear noise schedule of (1e − 6, 0.01) with 2000 timesteps during training, and the same noise schedule is used with 100 timesteps during inference within the DDIM framework. The clip range for TSDF sampling is [−3.0, 3.0].
7.3. PatchVQGAN for Learning the Occupancy Embedding
Network Structure. The network structure of PatchVQGAN described in Section 3.3 is shown in Fig. 11. The multi-scale encoding and decoding processes are slightly coupled with each other, while we simplify the description of the whole model for better understanding in Section 3.3. The encoder and decoder are implemented hierarchically as ”Encoder 1”, ”Encoder 2”, ”Decoder 1”, and ”Decoder 2” as shown in Fig. 11 (b)-(e). The multi-layer feed-forward discriminator is omitted here. Different from [9], we use quantizers with GumbelSoftmax [20] which enables a differentiable discrete sampling. The size of codebook is 8192, with the embedding dimension of 4 as commonly adopted in [9][37].
Training & Inference Settings. The hyper parameters in Eq. (11) are initially set to λ1 = 1.0, λ2 = 0.2. Additionally, a dynamic weight adapting strategy as [9] is employed
1


Stage 2: B×8×128×128×32
SparseRes 64, 3, 1
SparseConv3d 64, 3, 1
SparseRes 64, 3, 1
DownSample 2
SparseRes 128, 3, 1
SparseRes 128, 3, 1
DownSample 2
SparseRes 512, 3, 1
SparseRes 512, 3, 1
SparseRes 512, 3, 1
SparseRes 512, 3, 1
SparseRes 512, 3, 1
SparseRes 512, 3, 1
UpSample 2
SparseRes 128, 3, 1
SparseRes 128, 3, 1
UpSample 2
SparseRes 64, 3, 1
SparseRes 64, 3, 1
SparseConv3d 1, 3, 1
Linear
ε
SelfAttention
SparseRes 256, 3, 1
SparseRes 256, 3, 1
DownSample 2
UpSample 2
SparseRes 256, 3, 1
SparseRes 256, 3, 1
Stage 3: B×9×128×128×128
Stage 2: B×4×128×128×32
Stage 3: B×1×128×128×128
Sinusoidal
Timestep Embedding
SparseRes 64, 3, 1
SparseConv3d 64, 3, 1
SparseRes 64, 3, 1
DownSample 2
SparseRes 128, 3, 1
SpatialTransformer
SparseRes 128, 3, 1
SpatialTransformer
DownSample 2
SparseRes 256, 3, 1
SparseRes 256, 3, 1
SparseRes 256, 3, 1
SpatialTransformer
SparseRes 256, 3, 1
SparseRes 256, 3, 1
SparseRes 256, 3, 1
UpSample 2
SparseRes 128, 3, 1
SpatialTransformer
SparseRes 128, 3, 1
SpatialTransformer
UpSample 2
SparseRes 64, 3, 1
SparseRes 64, 3, 1
SparseConv3d 1, 3, 1
Linear
Sinusoidal
Timestep Embedding
ε
B×4×64×64×16
B×4×64×64×16
(a) UNet structure in the Stage 1 of our cascaded diffusion.
(b) UNet structure in the Stage 2 and Stage 3 of our cascaded diffusion.
Figure 9. Noise prediction networks in our cascaded diffusion. In Stage 1, we use multiple Spatial Transformers as (a) shows. In Stage 2 and Stage 3, we use same network structure as (b), with only one attention layer in the middle of network.
Timestep Embedding
SparseConv3d C, 3, 1
SparseConv3d C, 3, 1
Input
SparseConv3d C, 1, 1
3D Positional Embedding
SelfAttention
SelfAttention
Linear
SparseConv3d C, 1, 1
Input
(a) SparseRes (b) Spatial Transformer
Figure 10. Sparse units widely used in our implementation of noise prediction network in sparse diffusion.
to control λ2. The network parameters are randomly initialized with normal distribution in training process, and we use the Adam optimizer with a learning rate of 1.0 × 10−5.
7.4. Local Fusion of Diffusion
The average fusion method mentioned in Section 3.4 is defined as follows.
Average Fusion. Suppose xtk(p) ∼ N (μtk(p), Σtk(p)), we
have:
xt(p) ∼ N ( 1
|G(p)|
X
k∈G(p)
μk
t (p), 1
|G(p)|2
X
k∈G(p)
Σk
t (p)).
(14) The rapidly decreasing variance impacts generation diversity and quality. We, therefore, propose a stochastic TSDF fusion algorithm.
7.5. User Study
We conduct two user studies on meshes from generation and reconstruction refinement in Section 4.1 and 4.3, which are slightly different. Generation. We use same metric as Text2Room [17]: Completeness and Perceptual. In every page of the survey, the users scores one scene from one method by 1-5 points on these 2 metrics. Then we take an average score on each method.
Reconstruction Refinement. We employ more metrics here, including details, completeness, plane quality, and
2


Conv3D 32, 3, 1
ResnetBlock 32, 3, 1
ResnetBlock 32, 3, 1
DownSample 2
ResnetBlock 64, 3, 1
ResnetBlock 64, 3, 1
DownSample 2
ResnetBlock 128, 3, 1
ResnetBlock 128, 3, 1
ResnetBlock 128, 3, 1
ResnetBlock 128, 3, 1
SelfAttention
Conv3D 4, 3, 1
Conv3D 256, 3, 1
ResnetBlock 256, 3, 1
ResnetBlock 256, 3, 1
DownSample 2
ResnetBlock 256, 3, 1
ResnetBlock 256, 3, 1
ResnetBlock 256, 3, 1
ResnetBlock256, 3, 1
SelfAttention
Conv3D 4, 3, 1
Conv3D 128, 3, 1
ResnetBlock 128, 3, 1
ResnetBlock 128, 3, 1
SelfAttention
ResnetBlock 128, 3, 1
ResnetBlock 128, 3, 1
UpSample 2
ResnetBlock 64, 3, 1
ResnetBlock 64, 3, 1
UpSample 2
ResnetBlock 32, 3, 1
ResnetBlock 32, 3, 1
Conv3D
1, 3, 1
Conv3D
128, 3, 1
ResnetBlock 128, 3, 1
ResnetBlock 64, 3, 1
ResnetBlock 32, 3, 1
Conv3D
1, 3, 1
Conv3D 256, 3, 1
ResnetBlock 256, 3, 1
ResnetBlock 256, 3, 1
SelfAttention
ResnetBlock 256, 3, 1
ResnetBlock 256, 3, 1
UpSample 2
ResnetBlock 256, 3, 1
ResnetBlock 256, 3, 1
Conv3D
4, 3, 1
ResnetBlock 256, 3, 1
ResnetBlock 256, 3, 1
Conv3D
128, 3, 1
Conv3D
1, 3, 1
(b) Encoder 1 (c) Encoder 2
(d) Decoder 1 (e) Decoder 2
(a) Encoding-Decoding pipeline of PatchVQGAN
Encoder 1
Encoder 2
Quantizer 1
Quantizer 2
Decoder 1
Decoder 2
Conv3D 4, 1, 1
Conv3D 4, 1, 1
Conv3D 4, 1, 1
UpSample
Conv3D 4, 1, 1
x, B×1×96×96×96
Element-wise Multiplication
z("),
B×4×12×12×12
z($),
B×4×24×24×24
x+, B×1×96×96×96
(Binary Occupancy Mask)
,
B×1×96×96×96
TSDF Volume
Mz ̂(2)
M̂x
Figure 11. Network structure of PatchVQGAN.
edge quality. To save the time of the users, we use ranking rather than scoring for each scene. The feedback score Si for the i-th scene is computed as
Si = 1
di
di
X
j=1
s(ri,j), (15)
where ri,j ∈ 1, 2, 3, 4 represents the ranking given by the j-th user for the i-th scene. The function s(r) = 4 − r converts the ranking into a score, with the r-th rank worth 4 − r score. di is the total number of valid feedbacks for the i-th scene. By summing up the scores across all scenes, we obtain the total score
S=
N
X
i=1
Si (16)
8. More Results on Scene Generation
We provide more scene generation samples as shown in Fig. 12 - Fig. 14. Fig. 12 is an additional comparison between our method and Text2Room [17]. Since the Poisson [21] reconstruction
can produce better results than pure Text2Room, we only show the results of ”Text2Room + Poisson”. Fig. 13 and Fig. 14 are generated scene samples of our method.
3


Outer Look
Without Ceiling
Texture
Geometry
Texture
Geometry
Text2Room + Poisson Ours
Figure 12. Comparison of Text2Room and our approach in larger views. As previous Fig. 5 shows, Poisson reconstruction significantly improves the performance of pure TextRoom, so that here we only demonstrate the results of Text2Room [17] + Poisson [21]. The textures of our results are produced by DreamSpace [51] as a post-processing of scene geometry generation.
4


Figure 13. More generation samples in columns.
5


Figure 14. More generation samples in columns.
6